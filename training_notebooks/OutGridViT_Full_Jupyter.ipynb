{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Szc_5HDk-up"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import RandAugment\n",
        "\n",
        "def get_cifar100_datasets(\n",
        "    data_dir: str = \"./data\",\n",
        "    val_split: float = 0.0,\n",
        "    ra_num_ops: int = 2,\n",
        "    ra_magnitude: int = 7,\n",
        "    random_erasing_p: float = 0.25,\n",
        "    erasing_scale=(0.02, 0.20),\n",
        "    erasing_ratio=(0.3, 3.3),\n",
        "    img_size: int = 32,):\n",
        "\n",
        "    \"\"\"\n",
        "    CIFAR-100 datasets con augmentations \"mix-friendly\":\n",
        "    diseñadas para complementar Mixup/CutMix (en el loop) sin pasarse.\n",
        "\n",
        "    img_size:\n",
        "      - 32 (default): CIFAR nativo.\n",
        "      - >32: upsample (p.ej. 64) para experimentos (más tokens/compute).\n",
        "    \"\"\"\n",
        "    if img_size < 32:\n",
        "        raise ValueError(f\"img_size must be >= 32 for CIFAR-100. Got {img_size}.\")\n",
        "\n",
        "    cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "    cifar100_std  = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # Si subimos resolución, primero hacemos resize y adaptamos crop/padding.\n",
        "    # Padding recomendado proporcional: 32->4, 64->8, etc.\n",
        "\n",
        "    crop_padding = max(4, img_size // 8)\n",
        "\n",
        "    train_ops = []\n",
        "    if img_size != 32:\n",
        "        train_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n",
        "\n",
        "    train_ops += [\n",
        "        transforms.RandomCrop(img_size, padding=crop_padding),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        RandAugment(num_ops=ra_num_ops, magnitude=ra_magnitude),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar100_mean, cifar100_std),\n",
        "        transforms.RandomErasing(\n",
        "            p=random_erasing_p,\n",
        "            scale=erasing_scale,\n",
        "            ratio=erasing_ratio,\n",
        "            value=\"random\",),]\n",
        "\n",
        "    train_transform = transforms.Compose(train_ops)\n",
        "\n",
        "    test_ops = []\n",
        "    if img_size != 32:\n",
        "        test_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n",
        "\n",
        "    test_ops += [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar100_mean, cifar100_std),]\n",
        "\n",
        "    test_transform = transforms.Compose(test_ops)\n",
        "\n",
        "    full_train_dataset = datasets.CIFAR100(\n",
        "        root=data_dir, train=True, download=True, transform=train_transform)\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(\n",
        "        root=data_dir, train=False, download=True, transform=test_transform)\n",
        "\n",
        "    if val_split > 0.0:\n",
        "        n_total = len(full_train_dataset)\n",
        "        n_val = int(n_total * val_split)\n",
        "        n_train = n_total - n_val\n",
        "        train_dataset, val_dataset = random_split(\n",
        "            full_train_dataset,\n",
        "            [n_train, n_val],\n",
        "            generator=torch.Generator().manual_seed(7),)\n",
        "\n",
        "    else:\n",
        "        train_dataset = full_train_dataset\n",
        "        val_dataset = None\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "def get_cifar100_dataloaders(\n",
        "    batch_size: int = 128,\n",
        "    data_dir: str = \"./data\",\n",
        "    num_workers: int = 2,\n",
        "    val_split: float = 0.0,\n",
        "    pin_memory: bool = True,\n",
        "    ra_num_ops: int = 2,\n",
        "    ra_magnitude: int = 7,\n",
        "    random_erasing_p: float = 0.25,\n",
        "    img_size: int = 32,):\n",
        "    \"\"\"\n",
        "    Dataloaders CIFAR-100 listos para entrenar con Mixup/CutMix en el loop.\n",
        "    Augmentations no tan agresivas.\n",
        "\n",
        "    img_size:\n",
        "      - 32 (default): CIFAR nativo.\n",
        "      - 64: experimento de upsample (ojo: más compute).\n",
        "    \"\"\"\n",
        "    train_ds, val_ds, test_ds = get_cifar100_datasets(\n",
        "        data_dir=data_dir,\n",
        "        val_split=val_split,\n",
        "        ra_num_ops=ra_num_ops,\n",
        "        ra_magnitude=ra_magnitude,\n",
        "        random_erasing_p=random_erasing_p,\n",
        "        img_size=img_size,)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=(num_workers > 0),)\n",
        "\n",
        "    val_loader = None\n",
        "    if val_ds is not None:\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            persistent_workers=(num_workers > 0),)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=(num_workers > 0),)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcvYzk_fX9zK",
        "outputId": "11459783-ef9c-40a5-f5f4-fe004027058e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:19<00:00, 8.61MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader, test_loader = get_cifar100_dataloaders(\n",
        "    batch_size=64,\n",
        "    data_dir=\"./data/cifar100\",\n",
        "    num_workers=2,\n",
        "    val_split=0.1,\n",
        "    img_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUbtacnUYMKF",
        "outputId": "fc292ab8-01f3-4839-e8fd-ea199ded365d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "TRAIN_LOADER SUMMARY\n",
            "==========================================================================================\n",
            "Dataset type        : Subset\n",
            "  ↳ Wrapped dataset  : CIFAR100 (Subset-like)\n",
            "  ↳ Subset size      : 45000\n",
            "Num samples         : 45000\n",
            "Batch size          : 128\n",
            "Num workers         : 2\n",
            "Pin memory          : True\n",
            "Drop last           : False\n",
            "Sampler             : RandomSampler\n",
            "len(loader) (#batches): 352 (≈ ceil(45000/128) = 352)\n",
            "\n",
            "First batch:\n",
            "  x.shape           : (128, 3, 32, 32)\n",
            "  y.shape           : (128,)\n",
            "  x.dtype           : torch.float32\n",
            "  y.dtype           : torch.int64\n",
            "  x.min/max         : -3.9943 / 3.9723\n",
            "  y.min/max         : 0 / 99\n",
            "  unique labels (batch): 71\n",
            "\n",
            "Quick stats over up to 50 batches:\n",
            "  Approx mean        : -0.281566\n",
            "  Approx std         : 1.129613\n",
            "  Seen label counts  : 100 classes (in sampled batches)\n",
            "  Top-5 labels       : [(45, 88), (27, 83), (42, 78), (49, 77), (64, 77)]\n",
            "\n",
            "Full dataset label distribution:\n",
            "  #classes detected  : 100\n",
            "  min/max per class  : 436 / 463\n",
            "  first 10 classes   : [(0, 457), (1, 439), (2, 448), (3, 455), (4, 446), (5, 447), (6, 451), (7, 457), (8, 448), (9, 453)]\n",
            "  balance check      : not perfectly balanced\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
        "CIFAR100_STD  = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "\n",
        "\n",
        "def describe_loader(loader, name=\"loader\", max_batches_for_stats=50):\n",
        "    ds = loader.dataset\n",
        "    n = len(ds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"{name.upper()} SUMMARY\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    print(f\"Dataset type        : {type(ds).__name__}\")\n",
        "    if hasattr(ds, \"dataset\") and hasattr(ds, \"indices\"):\n",
        "        print(f\"  ↳ Wrapped dataset  : {type(ds.dataset).__name__} (Subset-like)\")\n",
        "        print(f\"  ↳ Subset size      : {len(ds.indices)}\")\n",
        "\n",
        "    print(f\"Num samples         : {n}\")\n",
        "    print(f\"Batch size          : {getattr(loader, 'batch_size', None)}\")\n",
        "    print(f\"Num workers         : {getattr(loader, 'num_workers', None)}\")\n",
        "    print(f\"Pin memory          : {getattr(loader, 'pin_memory', None)}\")\n",
        "    print(f\"Drop last           : {getattr(loader, 'drop_last', None)}\")\n",
        "\n",
        "    sampler = getattr(loader, \"sampler\", None)\n",
        "    sampler_name = type(sampler).__name__ if sampler is not None else None\n",
        "    print(f\"Sampler             : {sampler_name}\")\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    bs = loader.batch_size if loader.batch_size is not None else \"?\"\n",
        "    approx_batches = math.ceil(n / loader.batch_size) if loader.batch_size else \"?\"\n",
        "    print(f\"len(loader) (#batches): {num_batches} (≈ ceil({n}/{bs}) = {approx_batches})\")\n",
        "\n",
        "    x, y = next(iter(loader))\n",
        "    print(\"\\nFirst batch:\")\n",
        "    print(f\"  x.shape           : {tuple(x.shape)}\")\n",
        "    print(f\"  y.shape           : {tuple(y.shape)}\")\n",
        "    print(f\"  x.dtype           : {x.dtype}\")\n",
        "    print(f\"  y.dtype           : {y.dtype}\")\n",
        "    print(f\"  x.min/max         : {float(x.min()):.4f} / {float(x.max()):.4f}\")\n",
        "    print(f\"  y.min/max         : {int(y.min())} / {int(y.max())}\")\n",
        "    print(f\"  unique labels (batch): {len(torch.unique(y))}\")\n",
        "    print(f\"\\nQuick stats over up to {max_batches_for_stats} batches:\")\n",
        "\n",
        "    n_seen = 0\n",
        "    sum_ = 0.0\n",
        "    sumsq_ = 0.0\n",
        "    class_counts = Counter()\n",
        "\n",
        "    for bi, (xb, yb) in enumerate(loader):\n",
        "        if bi >= max_batches_for_stats:\n",
        "            break\n",
        "        xb = xb.float()\n",
        "        n_pix = xb.numel()\n",
        "        sum_ += xb.sum().item()\n",
        "        sumsq_ += (xb * xb).sum().item()\n",
        "        n_seen += n_pix\n",
        "\n",
        "        class_counts.update(yb.tolist())\n",
        "\n",
        "    mean = sum_ / max(1, n_seen)\n",
        "    var = (sumsq_ / max(1, n_seen)) - mean**2\n",
        "    std = math.sqrt(max(0.0, var))\n",
        "\n",
        "    print(f\"  Approx mean        : {mean:.6f}\")\n",
        "    print(f\"  Approx std         : {std:.6f}\")\n",
        "    top5 = class_counts.most_common(5)\n",
        "    print(f\"  Seen label counts  : {len(class_counts)} classes (in sampled batches)\")\n",
        "    print(f\"  Top-5 labels       : {top5}\")\n",
        "\n",
        "    targets = None\n",
        "    if hasattr(ds, \"targets\"):\n",
        "        targets = ds.targets\n",
        "    elif hasattr(ds, \"labels\"):\n",
        "        targets = ds.labels\n",
        "    elif hasattr(ds, \"dataset\") and hasattr(ds.dataset, \"targets\") and hasattr(ds, \"indices\"):\n",
        "        base_targets = ds.dataset.targets\n",
        "        targets = [base_targets[i] for i in ds.indices]\n",
        "\n",
        "    if targets is not None:\n",
        "        full_counts = Counter(list(map(int, targets)))\n",
        "        k = len(full_counts)\n",
        "        print(f\"\\nFull dataset label distribution:\")\n",
        "        print(f\"  #classes detected  : {k}\")\n",
        "        if k > 0:\n",
        "            mn = min(full_counts.values())\n",
        "            mx = max(full_counts.values())\n",
        "            print(f\"  min/max per class  : {mn} / {mx}\")\n",
        "            first10 = sorted(full_counts.items(), key=lambda t: t[0])[:10]\n",
        "            print(f\"  first 10 classes   : {first10}\")\n",
        "            if mn == mx:\n",
        "                print(\"  balance check      : perfectly balanced\")\n",
        "            else:\n",
        "                print(\"  balance check      : not perfectly balanced\")\n",
        "    else:\n",
        "        print(\"\\nFull dataset label distribution: (couldn't find targets/labels attribute)\")\n",
        "\n",
        "    print(\"=\"*90)\n",
        "\n",
        "\n",
        "describe_loader(train_loader, \"train_loader\", max_batches_for_stats=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u08t_5gzYXoN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def unnormalize(images: torch.Tensor,\n",
        "                mean=CIFAR100_MEAN,\n",
        "                std=CIFAR100_STD):\n",
        "    \"\"\"\n",
        "    Des-normaliza un batch de imágenes.\n",
        "    images: tensor [B, C, H, W] normalizado.\n",
        "    \"\"\"\n",
        "    mean = torch.tensor(mean, device=images.device).view(1, -1, 1, 1)\n",
        "    std = torch.tensor(std, device=images.device).view(1, -1, 1, 1)\n",
        "    return images * std + mean\n",
        "\n",
        "\n",
        "def show_batch(images: torch.Tensor,\n",
        "               labels: torch.Tensor,\n",
        "               class_names=None,\n",
        "               n: int = 8):\n",
        "    \"\"\"\n",
        "    Muestra las primeras n imágenes de un batch con sus labels.\n",
        "\n",
        "    Args:\n",
        "        images: tensor [B, C, H, W] (normalizado).\n",
        "        labels: tensor [B].\n",
        "        class_names: lista de nombres de clases (len = 100).\n",
        "        n: cuántas imágenes mostrar (en una fila).\n",
        "    \"\"\"\n",
        "    images = images[:n].cpu()\n",
        "    labels = labels[:n].cpu()\n",
        "    images_unnorm = unnormalize(images)\n",
        "\n",
        "    grid = make_grid(images_unnorm, nrow=n, padding=2)\n",
        "    npimg = grid.permute(1, 2, 0).numpy()\n",
        "\n",
        "    plt.figure(figsize=(2 * n, 2.5))\n",
        "    plt.imshow(npimg)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    if class_names is not None:\n",
        "        title = \" | \".join(class_names[int(lbl)] for lbl in labels)\n",
        "        plt.title(title, fontsize=10)\n",
        "    plt.show()\n",
        "\n",
        "cifar100_train = datasets.CIFAR100(\n",
        "    root=\"./data/cifar100\",\n",
        "    train=True,\n",
        "    download=False)\n",
        "\n",
        "class_names = cifar100_train.classes\n",
        "images, labels = next(iter(train_loader))\n",
        "show_batch(images, labels, class_names=class_names, n=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnzAvcYaY_rK"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ll76DN0Hx6FR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, C, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(C, eps=eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [B,C,H,W] -> [B,H,W,C] -> LN -> [B,C,H,W]\n",
        "        return self.ln(x.permute(0,2,3,1)).permute(0,3,1,2).contiguous()\n",
        "\n",
        "\n",
        "def _make_activation(act: str) -> nn.Module:\n",
        "    act = act.lower()\n",
        "    if act == \"silu\":\n",
        "        return nn.SiLU(inplace=True)\n",
        "    if act == \"relu\":\n",
        "        return nn.ReLU(inplace=True)\n",
        "    if act == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    raise ValueError(f\"Unknown activation '{act}'. Use one of: silu|gelu|relu\")\n",
        "\n",
        "\n",
        "class MLP2d(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0, act=\"gelu\"):\n",
        "        super().__init__()\n",
        "        hidden = max(1, int(dim * mlp_ratio))\n",
        "        self.fc1 = nn.Conv2d(dim, hidden, 1)\n",
        "        self.act = _make_activation(act)\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Conv2d(hidden, dim, 1)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class OutlookAttention2d(nn.Module):\n",
        "    \"\"\"\n",
        "    OutlookAttention on [B,C,H,W] (NCHW) with dynamic local aggregation.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 6,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "        qkv_bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
        "        if kernel_size <= 0 or kernel_size % 2 == 0:\n",
        "            raise ValueError(\"kernel_size must be odd and >0 (e.g., 3,5,7)\")\n",
        "        if stride <= 0:\n",
        "            raise ValueError(\"stride must be > 0\")\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        kk = kernel_size * kernel_size\n",
        "        bias = bool(qkv_bias)\n",
        "\n",
        "        # logits per spatial position\n",
        "        self.attn = nn.Conv2d(dim, num_heads * kk, kernel_size=1, bias=bias)\n",
        "        # values\n",
        "        self.v = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Conv2d(dim, dim, kernel_size=1, bias=True)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, H, W = x.shape\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        heads = self.num_heads\n",
        "        hd = self.head_dim\n",
        "        kk = k * k\n",
        "\n",
        "        # attn logits: [B, heads*kk, H, W] -> (optional) pool if stride>1\n",
        "        a = self.attn(x)\n",
        "        if s > 1:\n",
        "            a = F.avg_pool2d(a, kernel_size=s, stride=s)\n",
        "        _, _, Hs, Ws = a.shape\n",
        "\n",
        "        # [B, heads, kk, Hs, Ws] -> [B, Hs*Ws, heads, kk]\n",
        "        a = a.view(B, heads, kk, Hs, Ws).flatten(3).permute(0, 3, 1, 2).contiguous()\n",
        "        a = F.softmax(a, dim=-1)\n",
        "        a = self.attn_drop(a)\n",
        "\n",
        "        # values + unfold neighborhoods\n",
        "        v = self.v(x)  # [B,C,H,W]\n",
        "        pad = k // 2\n",
        "        v_unf = F.unfold(v, kernel_size=k, padding=pad, stride=s)  # [B, C*kk, Hs*Ws]\n",
        "\n",
        "        # -> [B, Hs*Ws, heads, hd, kk]\n",
        "        v_unf = v_unf.view(B, heads, hd, kk, Hs * Ws).permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        # weighted sum over kk\n",
        "        y = (v_unf * a.unsqueeze(3)).sum(dim=-1)  # [B, Hs*Ws, heads, hd]\n",
        "        y = y.permute(0, 2, 3, 1).contiguous().view(B, C, Hs, Ws)\n",
        "\n",
        "        y = self.proj(y)\n",
        "        y = self.proj_drop(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "At2AT2hN0hw8"
      },
      "outputs": [],
      "source": [
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    DropPath / Stochastic Depth. Works for any tensor shape with batch in dim 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.drop_prob = float(drop_prob)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.drop_prob == 0.0 or (not self.training):\n",
        "            return x\n",
        "        keep_prob = 1.0 - self.drop_prob\n",
        "        # shape: [B, 1, 1, 1, ...]\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        mask = torch.empty(shape, device=x.device, dtype=x.dtype).bernoulli_(keep_prob)\n",
        "        return x * mask / keep_prob\n",
        "\n",
        "\n",
        "class OutlookerBlock2d(nn.Module):\n",
        "    \"\"\"\n",
        "    x (NCHW) -> LN2d -> OutlookAttention2d -> DropPath + res\n",
        "             -> LN2d -> MLP2d            -> DropPath + res\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        mlp_ratio: float = 2.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "        drop_path: float = 0.0,\n",
        "        mlp_drop: float = 0.0,\n",
        "        act: str = \"gelu\",\n",
        "        norm_eps: float = 1e-6):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm2d(dim, eps=norm_eps)\n",
        "        self.attn = OutlookAttention2d(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=proj_drop)\n",
        "\n",
        "        self.dp1 = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        self.norm2 = LayerNorm2d(dim, eps=norm_eps)\n",
        "        self.mlp = MLP2d(dim=dim, mlp_ratio=mlp_ratio, drop=mlp_drop, act=act)\n",
        "        self.dp2 = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.dp1(self.attn(self.norm1(x)))\n",
        "        x = x + self.dp2(self.mlp(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_InvOziU0nKb",
        "outputId": "17a209dc-f3c8-4129-d847-0592f357eeaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 96, 16, 16]) torch.Size([8, 96, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(8, 96, 16, 16)\n",
        "blk = OutlookerBlock2d(dim=96, num_heads=6, kernel_size=3, stride=1)\n",
        "y = blk(x)\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fV4IUh9G222y"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from dataclasses import dataclass\n",
        "\n",
        "class SqueezeExcite(nn.Module):\n",
        "    def __init__(self, channels: int, se_ratio: float = 0.25, act: str = \"silu\"):\n",
        "        super().__init__()\n",
        "        if not (0.0 < se_ratio <= 1.0):\n",
        "            raise ValueError(\"se_ratio must be in (0, 1].\")\n",
        "\n",
        "        hidden = max(1, int(channels * se_ratio))\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=True)\n",
        "        self.act = _make_activation(act)\n",
        "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=True)\n",
        "        self.gate = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        s = self.pool(x)\n",
        "        s = self.fc1(s)\n",
        "        s = self.act(s)\n",
        "        s = self.fc2(s)\n",
        "        return x * self.gate(s)\n",
        "\n",
        "\n",
        "ActType = Literal[\"silu\", \"gelu\", \"relu\"]\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MBConvConfig:\n",
        "    expand_ratio: float = 4.0\n",
        "    se_ratio: float = 0.25\n",
        "    act: ActType = \"silu\"\n",
        "    use_bn: bool = True\n",
        "    drop_path: float = 0.0\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    \"\"\"\n",
        "    MBConv block (NCHW):\n",
        "      Expand 1x1 -> Depthwise 3x3 -> SE -> Project 1x1\n",
        "      Residual if stride=1 and in_ch==out_ch\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1, cfg: MBConvConfig = MBConvConfig()):\n",
        "        super().__init__()\n",
        "        if in_ch <= 0 or out_ch <= 0:\n",
        "            raise ValueError(\"in_ch and out_ch must be > 0\")\n",
        "        if stride not in (1, 2):\n",
        "            raise ValueError(\"stride must be 1 or 2\")\n",
        "\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.stride = stride\n",
        "\n",
        "        bn = (lambda c: nn.BatchNorm2d(c)) if cfg.use_bn else (lambda c: nn.Identity())\n",
        "        act = _make_activation(cfg.act)\n",
        "\n",
        "        mid_ch = max(1, int(round(in_ch * cfg.expand_ratio)))\n",
        "\n",
        "        if mid_ch != in_ch:\n",
        "            self.expand = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, mid_ch, kernel_size=1, bias=not cfg.use_bn),\n",
        "                bn(mid_ch),\n",
        "                act,)\n",
        "\n",
        "        else:\n",
        "            self.expand = nn.Identity()\n",
        "\n",
        "        self.depthwise = nn.Sequential(\n",
        "            nn.Conv2d(mid_ch, mid_ch, kernel_size=3, stride=stride, padding=1,\n",
        "                      groups=mid_ch, bias=not cfg.use_bn),\n",
        "            bn(mid_ch),\n",
        "            act,)\n",
        "\n",
        "        self.se = SqueezeExcite(mid_ch, se_ratio=cfg.se_ratio, act=cfg.act) if cfg.se_ratio > 0 else nn.Identity()\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=1, bias=not cfg.use_bn),\n",
        "            bn(out_ch),)\n",
        "\n",
        "        self.use_res = (stride == 1 and in_ch == out_ch)\n",
        "        self.drop_path = DropPath(cfg.drop_path) if (cfg.drop_path and cfg.drop_path > 0) else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.expand(x)\n",
        "        out = self.depthwise(out)\n",
        "        out = self.se(out)\n",
        "        out = self.project(out)\n",
        "\n",
        "        if self.use_res:\n",
        "            out = x + self.drop_path(out)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQXRj5I3aIq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-3CfVPEG3Z0Q"
      },
      "outputs": [],
      "source": [
        "def grid_partition(x: torch.Tensor, grid_size: int):\n",
        "    if x.ndim != 4:\n",
        "        raise ValueError(f\"Expected x.ndim==4 (BHWC). Got shape {tuple(x.shape)}\")\n",
        "    B, H, W, C = x.shape\n",
        "    g = grid_size\n",
        "    if g <= 0:\n",
        "        raise ValueError(\"grid_size must be > 0\")\n",
        "    if (H % g) != 0 or (W % g) != 0:\n",
        "        raise ValueError(f\"H and W must be divisible by grid_size. Got H={H}, W={W}, g={g}\")\n",
        "\n",
        "    Hg, Wg = H // g, W // g\n",
        "    x = x.view(B, Hg, g, Wg, g, C)  # [B, Hg, g, Wg, g, C]\n",
        "    grids = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(B * g * g, Hg, Wg, C)\n",
        "    meta = (B, H, W, C, g)\n",
        "    return grids, meta\n",
        "\n",
        "\n",
        "def grid_unpartition(grids: torch.Tensor, meta) -> torch.Tensor:\n",
        "    if grids.ndim != 4:\n",
        "        raise ValueError(f\"Expected grids.ndim==4. Got shape {tuple(grids.shape)}\")\n",
        "    B, H, W, C, g = meta\n",
        "    Hg, Wg = H // g, W // g\n",
        "    if grids.shape[0] != B * g * g:\n",
        "        raise ValueError(f\"grids.shape[0] must be B*g*g = {B*g*g}. Got {grids.shape[0]}\")\n",
        "    if grids.shape[1] != Hg or grids.shape[2] != Wg or grids.shape[3] != C:\n",
        "        raise ValueError(f\"grids shape mismatch. Expected (*,{Hg},{Wg},{C}) got {tuple(grids.shape)}\")\n",
        "\n",
        "    x = grids.view(B, g, g, Hg, Wg, C)\n",
        "    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(B, H, W, C)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vhrwKG8K3cJU"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "AttnMode = Literal[\"grid\"]\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class AttentionConfig:\n",
        "    dim: int\n",
        "    num_heads: int\n",
        "    qkv_bias: bool = True\n",
        "    attn_drop: float = 0.0\n",
        "    proj_drop: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class LocalAttention2DConfig:\n",
        "    mode: AttnMode\n",
        "    dim: int\n",
        "    num_heads: int\n",
        "    grid_size: int\n",
        "    window_size: int = 1\n",
        "    qkv_bias: bool = True\n",
        "    attn_drop: float = 0.0\n",
        "    proj_drop: float = 0.0\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard MHSA for token sequences.\n",
        "\n",
        "    Input:  x [B, N, C]\n",
        "    Output: y [B, N, C]\n",
        "\n",
        "    Works for both window and grid partitions because both can be flattened to [Bgrp, N, C].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: AttentionConfig):\n",
        "        super().__init__()\n",
        "        if cfg.dim <= 0:\n",
        "            raise ValueError(\"cfg.dim must be > 0\")\n",
        "        if cfg.num_heads <= 0:\n",
        "            raise ValueError(\"cfg.num_heads must be > 0\")\n",
        "        if cfg.dim % cfg.num_heads != 0:\n",
        "            raise ValueError(f\"dim ({cfg.dim}) must be divisible by num_heads ({cfg.num_heads})\")\n",
        "\n",
        "        self.dim = cfg.dim\n",
        "        self.num_heads = cfg.num_heads\n",
        "        self.head_dim = cfg.dim // cfg.num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(cfg.dim, 3 * cfg.dim, bias=cfg.qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(cfg.attn_drop)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim, bias=True)\n",
        "        self.proj_drop = nn.Dropout(cfg.proj_drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim != 3:\n",
        "            raise ValueError(f\"Expected x.ndim==3 with shape [B, N, C]. Got {tuple(x.shape)}\")\n",
        "        B, N, C = x.shape\n",
        "        if C != self.dim:\n",
        "            raise ValueError(f\"Expected last dim C={self.dim}. Got C={C}\")\n",
        "\n",
        "        # qkv: [B, N, 3C] -> [B, N, 3, heads, head_dim] -> [3, B, heads, N, head_dim]\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # attention: [B, heads, N, N]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # out: [B, heads, N, head_dim] -> [B, N, C]\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LocalAttention2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Grid attention wrapper.\n",
        "\n",
        "    Input/Output: x BHWC [B,H,W,C] -> [B,H,W,C]\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: LocalAttention2DConfig):\n",
        "        super().__init__()\n",
        "        if cfg.mode != \"grid\":\n",
        "            raise ValueError(\"This minimal version only supports mode='grid'\")\n",
        "        self.cfg = cfg\n",
        "        self.mhsa = MultiHeadSelfAttention(\n",
        "            AttentionConfig(\n",
        "                dim=cfg.dim,\n",
        "                num_heads=cfg.num_heads,\n",
        "                qkv_bias=cfg.qkv_bias,\n",
        "                attn_drop=cfg.attn_drop,\n",
        "                proj_drop=cfg.proj_drop,))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim != 4:\n",
        "            raise ValueError(f\"Expected x.ndim==4 (BHWC). Got {tuple(x.shape)}\")\n",
        "        B, H, W, C = x.shape\n",
        "        if C != self.cfg.dim:\n",
        "            raise ValueError(f\"Expected C=={self.cfg.dim}. Got C={C}\")\n",
        "\n",
        "        g = self.cfg.grid_size\n",
        "        grids, meta = grid_partition(x, g)         # [B*g*g, Hg, Wg, C]\n",
        "        Bgrp, Hg, Wg, _ = grids.shape\n",
        "        tokens = grids.view(Bgrp, Hg * Wg, C)      # [Bgrp, N, C]\n",
        "        tokens = self.mhsa(tokens)\n",
        "        grids = tokens.view(Bgrp, Hg, Wg, C)\n",
        "        out = grid_unpartition(grids, meta)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V8D_s4nb18n2"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP para BHWC: aplica sobre el último dim C.\n",
        "    x: [..., C] -> [..., C]\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, mlp_ratio: float = 4.0, drop: float = 0.0, act: str = \"gelu\"):\n",
        "        super().__init__()\n",
        "        hidden = max(1, int(dim * mlp_ratio))\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = _make_activation(act)\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.shape[-1] != self.fc1.in_features:\n",
        "            raise ValueError(f\"MLP expected last dim={self.fc1.in_features}, got {x.shape[-1]}\")\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class OutGridBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Híbrido: Outlooker (local dinámico) -> MBConv -> Grid-MHSA -> MLP\n",
        "    Input/Output: [B, C, H, W]\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        C = cfg.dim\n",
        "\n",
        "        # Outlooker en NCHW\n",
        "        self.outlook = OutlookerBlock2d(\n",
        "            dim=C,\n",
        "            num_heads=cfg.outlook_heads,          # nuevo hyperparam\n",
        "            kernel_size=cfg.outlook_kernel,       # nuevo hyperparam\n",
        "            stride=1,\n",
        "            mlp_ratio=cfg.outlook_mlp_ratio,      # opcional, puedes fijar 0 o 2\n",
        "            attn_drop=cfg.attn_drop,\n",
        "            proj_drop=cfg.proj_drop,\n",
        "            mlp_drop=cfg.ffn_drop,\n",
        "            drop_path=cfg.drop_path,\n",
        "            act=cfg.mlp_act,)\n",
        "\n",
        "        # MBConv NCHW\n",
        "        self.mbconv = MBConv(\n",
        "            in_ch=C, out_ch=C, stride=1,\n",
        "            cfg=MBConvConfig(\n",
        "                expand_ratio=cfg.mbconv_expand_ratio,\n",
        "                se_ratio=cfg.mbconv_se_ratio,\n",
        "                act=cfg.mbconv_act,\n",
        "                use_bn=cfg.use_bn,\n",
        "                drop_path=0.0,\n",
        "            ),)\n",
        "\n",
        "        # Grid attention BHWC\n",
        "        self.norm2 = nn.LayerNorm(C)\n",
        "        self.grid_attn = LocalAttention2D(\n",
        "            LocalAttention2DConfig(\n",
        "                mode=\"grid\",\n",
        "                dim=C,\n",
        "                num_heads=cfg.num_heads,\n",
        "                window_size=cfg.window_size,\n",
        "                grid_size=cfg.grid_size,\n",
        "                qkv_bias=True,\n",
        "                attn_drop=cfg.attn_drop,\n",
        "                proj_drop=cfg.proj_drop,\n",
        "            ))\n",
        "        self.dp2 = DropPath(cfg.drop_path) if cfg.drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # 4) MLP BHWC\n",
        "        self.norm3 = nn.LayerNorm(C)\n",
        "        self.mlp = MLP(dim=C, mlp_ratio=cfg.mlp_ratio, drop=cfg.ffn_drop, act=cfg.mlp_act)\n",
        "        self.dp3 = DropPath(cfg.drop_path) if cfg.drop_path > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Outlooker + MBConv (NCHW)\n",
        "        x = self.outlook(x)\n",
        "        x = self.mbconv(x)\n",
        "\n",
        "        # to BHWC for grid + mlp\n",
        "        x_bhwc = x.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        y = self.norm2(x_bhwc)\n",
        "        y = self.grid_attn(y)\n",
        "        x_bhwc = x_bhwc + self.dp2(y)\n",
        "\n",
        "        y = self.norm3(x_bhwc)\n",
        "        y = self.mlp(y)\n",
        "        x_bhwc = x_bhwc + self.dp3(y)\n",
        "\n",
        "        # back to NCHW\n",
        "        return x_bhwc.permute(0, 3, 1, 2).contiguous()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPvZ6KnH4SEG"
      },
      "source": [
        "## Test del nuevo modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y9Hj9lsp4TqM"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DummyCfg:\n",
        "    dim: int = 96\n",
        "\n",
        "    # Outlooker\n",
        "    outlook_heads: int = 6\n",
        "    outlook_kernel: int = 3\n",
        "    outlook_mlp_ratio: float = 2.0\n",
        "\n",
        "    # MBConv\n",
        "    mbconv_expand_ratio: float = 4.0\n",
        "    mbconv_se_ratio: float = 0.25\n",
        "    mbconv_act: str = \"silu\"\n",
        "    use_bn: bool = True\n",
        "\n",
        "    # Grid MHSA\n",
        "    num_heads: int = 6\n",
        "    grid_size: int = 4\n",
        "    window_size: int = 8  # no se usa en grid-only, pero tu ctor lo pasa\n",
        "\n",
        "    # Drops\n",
        "    attn_drop: float = 0.0\n",
        "    proj_drop: float = 0.0\n",
        "    ffn_drop: float = 0.0\n",
        "    drop_path: float = 0.0\n",
        "\n",
        "    # MLP (BHWC)\n",
        "    mlp_ratio: float = 4.0\n",
        "    mlp_act: str = \"gelu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaIRHjm74Wd0"
      },
      "outputs": [],
      "source": [
        "def _assert_shape(x: torch.Tensor, shape: tuple, name: str = \"tensor\"):\n",
        "    assert tuple(x.shape) == tuple(shape), f\"{name}: expected shape {shape}, got {tuple(x.shape)}\"\n",
        "\n",
        "def _assert_ndim(x: torch.Tensor, ndim: int, name: str = \"tensor\"):\n",
        "    assert x.ndim == ndim, f\"{name}: expected ndim={ndim}, got ndim={x.ndim}, shape={tuple(x.shape)}\"\n",
        "\n",
        "def _assert_finite(x: torch.Tensor, name: str = \"tensor\"):\n",
        "    assert torch.isfinite(x).all().item(), f\"{name}: found non-finite values (nan/inf)\"\n",
        "\n",
        "def _assert_divisible_hw(H: int, W: int, g: int):\n",
        "    assert (H % g) == 0 and (W % g) == 0, f\"H,W must be divisible by grid_size g={g}. Got H={H}, W={W}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s7ajSLwC4YDU"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_outlooker_stage(block: OutGridBlock, x: torch.Tensor):\n",
        "    _assert_ndim(x, 4, \"x\")\n",
        "    B, C, H, W = x.shape\n",
        "    _assert_shape(x, (B, block.outlook.norm1.ln.normalized_shape[0], H, W), \"x (pre)\")  # C check\n",
        "\n",
        "    y = block.outlook(x)\n",
        "    _assert_shape(y, (B, C, H, W), \"outlook(x)\")\n",
        "    _assert_finite(y, \"outlook(x)\")\n",
        "    return y\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_mbconv_stage(block: OutGridBlock, x: torch.Tensor):\n",
        "    B, C, H, W = x.shape\n",
        "    y = block.mbconv(x)\n",
        "    _assert_shape(y, (B, C, H, W), \"mbconv(x)\")\n",
        "    _assert_finite(y, \"mbconv(x)\")\n",
        "    return y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_grid_stage(block: OutGridBlock, x_nchw: torch.Tensor):\n",
        "    B, C, H, W = x_nchw.shape\n",
        "    x_bhwc = x_nchw.permute(0, 2, 3, 1).contiguous()\n",
        "    _assert_shape(x_bhwc, (B, H, W, C), \"x_bhwc\")\n",
        "\n",
        "    # divisibilidad\n",
        "    g = block.grid_attn.cfg.grid_size\n",
        "    _assert_divisible_hw(H, W, g)\n",
        "\n",
        "    y = block.norm2(x_bhwc)\n",
        "    _assert_shape(y, (B, H, W, C), \"norm2(x_bhwc)\")\n",
        "    y = block.grid_attn(y)\n",
        "    _assert_shape(y, (B, H, W, C), \"grid_attn(norm2(x_bhwc))\")\n",
        "    _assert_finite(y, \"grid_attn output\")\n",
        "\n",
        "    out = x_bhwc + block.dp2(y)\n",
        "    _assert_shape(out, (B, H, W, C), \"residual after grid\")\n",
        "    _assert_finite(out, \"after grid residual\")\n",
        "\n",
        "    return out  # BHWC\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_mlp_stage(block: OutGridBlock, x_bhwc: torch.Tensor):\n",
        "    B, H, W, C = x_bhwc.shape\n",
        "\n",
        "    y = block.norm3(x_bhwc)\n",
        "    _assert_shape(y, (B, H, W, C), \"norm3(x_bhwc)\")\n",
        "    y = block.mlp(y)\n",
        "    _assert_shape(y, (B, H, W, C), \"mlp(norm3(x_bhwc))\")\n",
        "    _assert_finite(y, \"mlp output\")\n",
        "\n",
        "    out = x_bhwc + block.dp3(y)\n",
        "    _assert_shape(out, (B, H, W, C), \"residual after mlp\")\n",
        "    _assert_finite(out, \"after mlp residual\")\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_full_forward_matches_stages(block: OutGridBlock, x: torch.Tensor, atol=1e-6, rtol=1e-5):\n",
        "    block.eval()\n",
        "\n",
        "    # manual pipeline\n",
        "    a = test_outlooker_stage(block, x)\n",
        "    b = test_mbconv_stage(block, a)\n",
        "    c = test_grid_stage(block, b)         # BHWC\n",
        "    d = test_mlp_stage(block, c)          # BHWC\n",
        "    manual = d.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "    # direct forward\n",
        "    direct = block(x)\n",
        "\n",
        "    _assert_shape(direct, x.shape, \"block(x)\")\n",
        "    _assert_finite(direct, \"block(x)\")\n",
        "    assert torch.allclose(manual, direct, atol=atol, rtol=rtol), \\\n",
        "        \"Manual staged pipeline != block.forward output (check wiring/residuals/norms).\"\n",
        "\n",
        "    return direct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAkYN2aS4irq",
        "outputId": "0850564f-8d9b-4a50-88b5-8ffb18ca1c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed. y: torch.Size([2, 96, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "def run_all_tests():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    cfg = DummyCfg(dim=96, grid_size=4)\n",
        "    blk = OutGridBlock(cfg).eval()\n",
        "\n",
        "    x = torch.randn(2, 96, 16, 16)\n",
        "    assert x.shape[2] % cfg.grid_size == 0 and x.shape[3] % cfg.grid_size == 0\n",
        "\n",
        "    y = test_full_forward_matches_stages(blk, x)\n",
        "    print(\"All tests passed. y:\", y.shape)\n",
        "\n",
        "run_all_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffdc4VMM6DuG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gQ-aect-5ywp"
      },
      "outputs": [],
      "source": [
        "class MaxOutStage(nn.Module):\n",
        "    def __init__(self, block_cfg, depth: int):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([OutGridBlock(block_cfg) for _ in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "        return x\n",
        "\n",
        "class GridOnlyBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    MBConv -> Grid-MHSA -> MLP (sin window attn).\n",
        "    Input/Output: [B,C,H,W]\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        C = cfg.dim\n",
        "\n",
        "        self.mbconv = MBConv(\n",
        "            in_ch=C, out_ch=C, stride=1,\n",
        "            cfg=MBConvConfig(\n",
        "                expand_ratio=cfg.mbconv_expand_ratio,\n",
        "                se_ratio=cfg.mbconv_se_ratio,\n",
        "                act=cfg.mbconv_act,\n",
        "                use_bn=cfg.use_bn,\n",
        "                drop_path=0.0,\n",
        "            ))\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(C)\n",
        "        self.grid_attn = LocalAttention2D(\n",
        "            LocalAttention2DConfig(\n",
        "                mode=\"grid\",\n",
        "                dim=C,\n",
        "                num_heads=cfg.num_heads,\n",
        "                window_size=getattr(cfg, \"window_size\", 1),\n",
        "                grid_size=cfg.grid_size,\n",
        "                qkv_bias=True,\n",
        "                attn_drop=cfg.attn_drop,\n",
        "                proj_drop=cfg.proj_drop,))\n",
        "\n",
        "        self.dp2 = DropPath(cfg.drop_path) if cfg.drop_path > 0 else nn.Identity()\n",
        "\n",
        "        self.norm3 = nn.LayerNorm(C)\n",
        "        self.mlp = MLP(dim=C, mlp_ratio=cfg.mlp_ratio, drop=cfg.ffn_drop, act=cfg.mlp_act)\n",
        "        self.dp3 = DropPath(cfg.drop_path) if cfg.drop_path > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.mbconv(x)\n",
        "\n",
        "        x_bhwc = x.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        y = self.norm2(x_bhwc)\n",
        "        y = self.grid_attn(y)\n",
        "        x_bhwc = x_bhwc + self.dp2(y)\n",
        "\n",
        "        y = self.norm3(x_bhwc)\n",
        "        y = self.mlp(y)\n",
        "        x_bhwc = x_bhwc + self.dp3(y)\n",
        "\n",
        "        return x_bhwc.permute(0, 3, 1, 2).contiguous()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9OYFcY695-Ct"
      },
      "outputs": [],
      "source": [
        "class StageOutThenGrid(nn.Module):\n",
        "    \"\"\"\n",
        "    Outlooker una vez al inicio del stage, luego varios GridOnlyBlock.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg, depth: int, out_depth: int = 1):\n",
        "        super().__init__()\n",
        "        self.outlookers = nn.ModuleList([\n",
        "            OutlookerBlock2d(\n",
        "                dim=cfg.dim,\n",
        "                num_heads=cfg.outlook_heads,\n",
        "                kernel_size=cfg.outlook_kernel,\n",
        "                stride=1,\n",
        "                mlp_ratio=cfg.outlook_mlp_ratio,\n",
        "                attn_drop=cfg.attn_drop,\n",
        "                proj_drop=cfg.proj_drop,\n",
        "                mlp_drop=cfg.ffn_drop,\n",
        "                drop_path=cfg.drop_path,\n",
        "                act=cfg.mlp_act,)\n",
        "\n",
        "            for _ in range(out_depth)])\n",
        "\n",
        "        self.blocks = nn.ModuleList([GridOnlyBlock(cfg) for _ in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for o in self.outlookers:\n",
        "            x = o(x)\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJTafDZ-6Lpv"
      },
      "source": [
        "# DownSampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HTosN2946LkG"
      },
      "outputs": [],
      "source": [
        "DownsampleType = Literal[\"conv\", \"pool\"]\n",
        "ActType = Literal[\"silu\", \"gelu\", \"relu\"]\n",
        "\n",
        "def _make_activation(act) -> nn.Module:\n",
        "    act = act.lower()\n",
        "    if act == \"silu\":\n",
        "        return nn.SiLU(inplace=True)\n",
        "    if act == \"relu\":\n",
        "        return nn.ReLU(inplace=True)\n",
        "    if act == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    raise ValueError(f\"Unknown activation '{act}'. Use one of: silu|gelu|relu\")\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DownsampleConfig:\n",
        "    kind: DownsampleType = \"conv\"  # \"conv\" or \"pool\"\n",
        "    act: ActType = \"silu\"\n",
        "    use_bn: bool = True\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    Downsample block:\n",
        "      - \"conv\": Conv3x3 stride2 padding1 (in_ch -> out_ch) + BN + Act\n",
        "      - \"pool\": AvgPool2x2 + Conv1x1 (in_ch -> out_ch) + BN + Act\n",
        "\n",
        "    Input:  [B, in_ch, H, W]\n",
        "    Output: [B, out_ch, H/2, W/2]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_ch: int, out_ch: int, cfg: DownsampleConfig = DownsampleConfig()):\n",
        "        super().__init__()\n",
        "        if in_ch <= 0 or out_ch <= 0:\n",
        "            raise ValueError(\"in_ch and out_ch must be > 0\")\n",
        "\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.kind = cfg.kind\n",
        "\n",
        "        bn = (lambda c: nn.BatchNorm2d(c)) if cfg.use_bn else (lambda c: nn.Identity())\n",
        "        act = _make_activation(cfg.act)\n",
        "\n",
        "        if cfg.kind == \"conv\":\n",
        "            self.op = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=2, padding=1, bias=not cfg.use_bn),\n",
        "                bn(out_ch),\n",
        "                act,)\n",
        "        elif cfg.kind == \"pool\":\n",
        "            self.op = nn.Sequential(\n",
        "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=not cfg.use_bn),\n",
        "                bn(out_ch),\n",
        "                act,)\n",
        "        else:\n",
        "            raise ValueError(\"cfg.kind must be 'conv' or 'pool'\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.op(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t68ZJmMX6P8A"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JPh2vPeh7FEk"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class StageCfg:\n",
        "    # core dims\n",
        "    dim: int\n",
        "    depth: int\n",
        "\n",
        "    # grid attention\n",
        "    num_heads: int\n",
        "    grid_size: int\n",
        "    window_size: int = 8  # no se usa en grid-only, pero lo mantenemos compatible\n",
        "\n",
        "    # outlooker\n",
        "    outlook_heads: int = 6\n",
        "    outlook_kernel: int = 3\n",
        "    outlook_mlp_ratio: float = 2.0\n",
        "\n",
        "    # MBConv\n",
        "    mbconv_expand_ratio: float = 4.0\n",
        "    mbconv_se_ratio: float = 0.25\n",
        "    mbconv_act: str = \"silu\"\n",
        "    use_bn: bool = True\n",
        "\n",
        "    # drops\n",
        "    attn_drop: float = 0.0\n",
        "    proj_drop: float = 0.0\n",
        "    ffn_drop: float = 0.0\n",
        "    drop_path: float = 0.0\n",
        "\n",
        "    # MLP (BHWC)\n",
        "    mlp_ratio: float = 4.0\n",
        "    mlp_act: str = \"gelu\"\n",
        "\n",
        "\n",
        "def make_dpr(total_blocks: int, dpr_max: float) -> List[float]:\n",
        "    if total_blocks <= 1:\n",
        "        return [dpr_max]\n",
        "    return [dpr_max * i / (total_blocks - 1) for i in range(total_blocks)]\n",
        "\n",
        "\n",
        "class ConvStem(nn.Module):\n",
        "    def __init__(self, in_ch: int, out_ch: int, act: str = \"silu\", use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        bn = (lambda c: nn.BatchNorm2d(c)) if use_bn else (lambda c: nn.Identity())\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=not use_bn),\n",
        "            bn(out_ch),\n",
        "            _make_activation(act),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.stem(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JarHZhH77WcT"
      },
      "outputs": [],
      "source": [
        "class OutlookerFrontGridNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo A:\n",
        "      Stem -> OutlookerFront (L bloques) -> (Stage: GridOnlyBlock x depth + Downsample) -> Head\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        stages: List[StageCfg],\n",
        "        in_ch: int = 3,\n",
        "        stem_dim: int = 64,\n",
        "        outlooker_front_depth: int = 2,   # <- varios outlookers \"tipo VOLO\"\n",
        "        dpr_max: float = 0.1,\n",
        "        down_cfg: DownsampleConfig = DownsampleConfig(kind=\"conv\", act=\"silu\", use_bn=True),):\n",
        "\n",
        "        super().__init__()\n",
        "        assert len(stages) >= 1\n",
        "        self.stem = ConvStem(in_ch, stem_dim, act=\"silu\", use_bn=True)\n",
        "\n",
        "        # proyección para entrar a dim del stage1 si stem_dim != stage1.dim\n",
        "        self.proj_in = nn.Identity()\n",
        "        if stem_dim != stages[0].dim:\n",
        "            self.proj_in = nn.Conv2d(stem_dim, stages[0].dim, kernel_size=1, bias=True)\n",
        "\n",
        "        # schedule global de drop_path por bloque (front + sum(stage.depth))\n",
        "        total_blocks = outlooker_front_depth + sum(s.depth for s in stages)\n",
        "        dprs = make_dpr(total_blocks, dpr_max)\n",
        "        idx = 0\n",
        "\n",
        "        # Outlooker front (NCHW) con residual + DropPath interno\n",
        "        front_cfg = stages[0]\n",
        "        self.front = nn.ModuleList()\n",
        "        for _ in range(outlooker_front_depth):\n",
        "            c = front_cfg\n",
        "            self.front.append(\n",
        "                OutlookerBlock2d(\n",
        "                    dim=c.dim,\n",
        "                    num_heads=c.outlook_heads,\n",
        "                    kernel_size=c.outlook_kernel,\n",
        "                    stride=1,\n",
        "                    mlp_ratio=c.outlook_mlp_ratio,\n",
        "                    attn_drop=c.attn_drop,\n",
        "                    proj_drop=c.proj_drop,\n",
        "                    mlp_drop=c.ffn_drop,\n",
        "                    drop_path=dprs[idx],\n",
        "                    act=c.mlp_act,))\n",
        "\n",
        "            idx += 1\n",
        "\n",
        "        # stages: GridOnlyBlock stacks + downsample between stages\n",
        "        self.stages = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        for si, scfg in enumerate(stages):\n",
        "            blocks = nn.ModuleList()\n",
        "            for _ in range(scfg.depth):\n",
        "                # clonar cfg pero con drop_path asignado por bloque\n",
        "                bcfg = StageCfg(**{**scfg.__dict__, \"drop_path\": dprs[idx]})\n",
        "                blocks.append(GridOnlyBlock(bcfg))\n",
        "                idx += 1\n",
        "            self.stages.append(blocks)\n",
        "\n",
        "            # downsample (except after last stage)\n",
        "            if si < len(stages) - 1:\n",
        "                self.downs.append(Downsample(scfg.dim, stages[si+1].dim, cfg=down_cfg))\n",
        "\n",
        "        # head\n",
        "        self.head_norm = nn.BatchNorm2d(stages[-1].dim)\n",
        "        self.classifier = nn.Linear(stages[-1].dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.proj_in(x)\n",
        "\n",
        "        # front outlooker\n",
        "        for blk in self.front:\n",
        "            x = blk(x)\n",
        "\n",
        "        # grid-only stages\n",
        "        for si, blocks in enumerate(self.stages):\n",
        "            for blk in blocks:\n",
        "                x = blk(x)\n",
        "            if si < len(self.downs):\n",
        "                x = self.downs[si](x)\n",
        "\n",
        "        # global pool + cls\n",
        "        x = self.head_norm(x)\n",
        "        x = x.mean(dim=(2, 3))\n",
        "\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "STtT8-Jq7i16"
      },
      "outputs": [],
      "source": [
        "class MaxOutNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo B:\n",
        "      Stem -> (Stage: MaxOutBlock x depth + Downsample) -> Head\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        stages: List[StageCfg],\n",
        "        in_ch: int = 3,\n",
        "        stem_dim: int = 64,\n",
        "        dpr_max: float = 0.1,\n",
        "        down_cfg: DownsampleConfig = DownsampleConfig(kind=\"conv\", act=\"silu\", use_bn=True),):\n",
        "\n",
        "        super().__init__()\n",
        "        assert len(stages) >= 1\n",
        "        self.stem = ConvStem(in_ch, stem_dim, act=\"silu\", use_bn=True)\n",
        "\n",
        "        self.proj_in = nn.Identity()\n",
        "\n",
        "        if stem_dim != stages[0].dim:\n",
        "            self.proj_in = nn.Conv2d(stem_dim, stages[0].dim, kernel_size=1, bias=True)\n",
        "\n",
        "        total_blocks = sum(s.depth for s in stages)\n",
        "        dprs = make_dpr(total_blocks, dpr_max)\n",
        "        idx = 0\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        for si, scfg in enumerate(stages):\n",
        "            blocks = nn.ModuleList()\n",
        "            for _ in range(scfg.depth):\n",
        "                bcfg = StageCfg(**{**scfg.__dict__, \"drop_path\": dprs[idx]})\n",
        "\n",
        "                blocks.append(OutGridBlock(bcfg))\n",
        "                idx += 1\n",
        "            self.stages.append(blocks)\n",
        "\n",
        "            if si < len(stages) - 1:\n",
        "                self.downs.append(Downsample(scfg.dim, stages[si+1].dim, cfg=down_cfg))\n",
        "\n",
        "        self.head_norm = nn.BatchNorm2d(stages[-1].dim)\n",
        "        self.classifier = nn.Linear(stages[-1].dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.proj_in(x)\n",
        "\n",
        "        for si, blocks in enumerate(self.stages):\n",
        "            for blk in blocks:\n",
        "                x = blk(x)\n",
        "            if si < len(self.downs):\n",
        "                x = self.downs[si](x)\n",
        "\n",
        "        x = self.head_norm(x)\n",
        "        x = x.mean(dim=(2, 3))\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC8-JW5-7ryG"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrUgyIa-7tH7"
      },
      "outputs": [],
      "source": [
        "def cifar64_stages_tiny():\n",
        "    # resoluciones esperadas: 64 -> 32 -> 16 -> 8 -> 4\n",
        "    '''\n",
        "    dim: #canales del feature map en ese stage\n",
        "    depth: cuántos bloques repites en ese stage\n",
        "    num_heads: #heads de la Grid-MHSA en ese stage\n",
        "    grid_size: cómo se parte la imagen para grid attention (debe dividir H y W)\n",
        "    outlook_heads: #heads del Outlooker (si ese modelo lo usa en ese stage)\n",
        "    '''\n",
        "\n",
        "    return [\n",
        "        StageCfg(dim=96,  depth=2, num_heads=3, grid_size=8, outlook_heads=3),\n",
        "        StageCfg(dim=192, depth=2, num_heads=6, grid_size=8, outlook_heads=6),\n",
        "        StageCfg(dim=384, depth=5, num_heads=12, grid_size=4, outlook_heads=12),\n",
        "        StageCfg(dim=768, depth=2, num_heads=12, grid_size=2, outlook_heads=12),]\n",
        "\n",
        "\n",
        "stages = cifar64_stages_tiny()\n",
        "\n",
        "mA = OutlookerFrontGridNet(num_classes=100, stages=stages, stem_dim=96, outlooker_front_depth=2, dpr_max=0.1)\n",
        "mB = MaxOutNet(num_classes=100, stages=stages, stem_dim=96, dpr_max=0.1)\n",
        "\n",
        "x = torch.randn(2, 3, 64, 64)\n",
        "yA = mA(x)\n",
        "yB = mB(x)\n",
        "print(yA.shape, yB.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_91feMK85Xy"
      },
      "source": [
        "---\n",
        "\n",
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G9XteOq588JY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from typing import Optional, Dict, Tuple, Any\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed: int = 0, deterministic: bool = False):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    else:\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "DTYPE_MAP = {\n",
        "    \"bf16\": torch.bfloat16, \"bfloat16\": torch.bfloat16,\n",
        "    \"fp16\": torch.float16,  \"float16\": torch.float16,\n",
        "    \"fp32\": torch.float32,  \"float32\": torch.float32,}\n",
        "\n",
        "def _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n",
        "    if not torch.cuda.is_available():\n",
        "        return False\n",
        "    return dtype in (torch.float16, torch.bfloat16)\n",
        "\n",
        "def make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n",
        "    if not enabled:\n",
        "        return None\n",
        "\n",
        "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n",
        "        try:\n",
        "            sig = inspect.signature(torch.amp.GradScaler)\n",
        "            if len(sig.parameters) >= 1:\n",
        "                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n",
        "            return torch.amp.GradScaler()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n",
        "        return torch.cuda.amp.GradScaler()\n",
        "    return None\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def autocast_ctx(\n",
        "    device: str = \"cuda\",\n",
        "    enabled: bool = True,\n",
        "    dtype: str = \"fp16\",\n",
        "    cache_enabled: bool = True,):\n",
        "    \"\"\"\n",
        "    Context manager de autocast:\n",
        "      - cuda: fp16 por defecto (ideal en T4)\n",
        "      - cpu: bfloat16 si está disponible\n",
        "    \"\"\"\n",
        "    if not enabled:\n",
        "        with nullcontext():\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        want = DTYPE_MAP.get(dtype.lower(), torch.float16)\n",
        "        use = want if _cuda_dtype_supported(want) else torch.float16\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        try:\n",
        "            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n",
        "                yield\n",
        "        except Exception:\n",
        "            with nullcontext():\n",
        "                yield\n",
        "        return\n",
        "\n",
        "    with nullcontext():\n",
        "        yield"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kYQ732DP9CCM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_checkpoint(\n",
        "    path: str,\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    scaler,\n",
        "    epoch: int,\n",
        "    best_top1: float,\n",
        "    extra: dict | None = None,):\n",
        "\n",
        "    ckpt = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n",
        "        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
        "        \"scaler\": scaler.state_dict() if scaler is not None else None,\n",
        "        \"epoch\": epoch,\n",
        "        \"best_top1\": best_top1,\n",
        "        \"extra\": extra or {},}\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "\n",
        "def load_checkpoint(\n",
        "    path: str,\n",
        "    model,\n",
        "    optimizer=None,\n",
        "    scheduler=None,\n",
        "    scaler=None,\n",
        "    map_location=\"cpu\",\n",
        "    strict: bool = True,):\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=strict)\n",
        "\n",
        "    if optimizer is not None and ckpt.get(\"optimizer\") is not None:\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    if scheduler is not None and ckpt.get(\"scheduler\") is not None:\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "    if scaler is not None and ckpt.get(\"scaler\") is not None:\n",
        "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "    return ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "toJSWec19P2s"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def accuracy_topk(logits: torch.Tensor, targets: torch.Tensor, ks=(1, 3, 5)) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    targets can be:\n",
        "      - int64 class indices [B]\n",
        "      - soft targets [B, num_classes] (we'll argmax for accuracy reporting)\n",
        "    \"\"\"\n",
        "    if targets.ndim == 2:\n",
        "        targets = targets.argmax(dim=1)\n",
        "\n",
        "    max_k = max(ks)\n",
        "    B = targets.size(0)\n",
        "    _, pred = torch.topk(logits, k=max_k, dim=1)\n",
        "    correct = pred.eq(targets.view(-1, 1).expand_as(pred))\n",
        "    out = {}\n",
        "    for k in ks:\n",
        "        out[k] = 100.0 * correct[:, :k].any(dim=1).float().sum().item() / B\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zvJc7YKo9UdD"
      },
      "outputs": [],
      "source": [
        "def _one_hot(targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
        "    return F.one_hot(targets, num_classes=num_classes).float()\n",
        "\n",
        "\n",
        "def soft_target_cross_entropy(logits: torch.Tensor, targets_soft: torch.Tensor) -> torch.Tensor:\n",
        "    logp = F.log_softmax(logits, dim=1)\n",
        "    return -(targets_soft * logp).sum(dim=1).mean()\n",
        "\n",
        "\n",
        "def apply_mixup_cutmix(\n",
        "    images: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    num_classes: int,\n",
        "    mixup_alpha: float = 0.0,\n",
        "    cutmix_alpha: float = 0.0,\n",
        "    prob: float = 1.0,):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      images_aug: [B,3,H,W]\n",
        "      targets_soft: [B,K]\n",
        "    \"\"\"\n",
        "    if prob <= 0.0 or (mixup_alpha <= 0.0 and cutmix_alpha <= 0.0):\n",
        "        return images, _one_hot(targets, num_classes)\n",
        "\n",
        "    if random.random() > prob:\n",
        "        return images, _one_hot(targets, num_classes)\n",
        "\n",
        "    use_cutmix = (cutmix_alpha > 0.0) and (mixup_alpha <= 0.0 or random.random() < 0.5)\n",
        "    B, _, H, W = images.shape\n",
        "    perm = torch.randperm(B, device=images.device)\n",
        "\n",
        "    y1 = _one_hot(targets, num_classes)\n",
        "    y2 = _one_hot(targets[perm], num_classes)\n",
        "\n",
        "    if use_cutmix:\n",
        "        lam = torch.distributions.Beta(cutmix_alpha, cutmix_alpha).sample().item()\n",
        "        cut_w = int(W * math.sqrt(1.0 - lam))\n",
        "        cut_h = int(H * math.sqrt(1.0 - lam))\n",
        "        cx = random.randint(0, W - 1)\n",
        "        cy = random.randint(0, H - 1)\n",
        "\n",
        "        x1 = max(cx - cut_w // 2, 0)\n",
        "        x2 = min(cx + cut_w // 2, W)\n",
        "        y1b = max(cy - cut_h // 2, 0)\n",
        "        y2b = min(cy + cut_h // 2, H)\n",
        "\n",
        "        images_aug = images.clone()\n",
        "        images_aug[:, :, y1b:y2b, x1:x2] = images[perm, :, y1b:y2b, x1:x2]\n",
        "\n",
        "        # adjust lambda based on actual area swapped\n",
        "        area = (x2 - x1) * (y2b - y1b)\n",
        "        lam = 1.0 - area / float(W * H)\n",
        "    else:\n",
        "        lam = torch.distributions.Beta(mixup_alpha, mixup_alpha).sample().item()\n",
        "        images_aug = images * lam + images[perm] * (1.0 - lam)\n",
        "\n",
        "    targets_soft = y1 * lam + y2 * (1.0 - lam)\n",
        "    return images_aug, targets_soft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qO-QWNsY9XU7"
      },
      "outputs": [],
      "source": [
        "def build_param_groups_no_wd(model: nn.Module, weight_decay: float):\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "\n",
        "        name_l = name.lower()\n",
        "        # no decay for biases + norms + positional/class tokens\n",
        "        if (\n",
        "            name.endswith(\".bias\")\n",
        "            or (\"norm\" in name_l)\n",
        "            or (\"bn\" in name_l)\n",
        "            or (\"ln\" in name_l)\n",
        "            or (\"pos\" in name_l)         # pos_embed / pos\n",
        "            or (\"cls_token\" in name_l)\n",
        "        ):\n",
        "            no_decay.append(p)\n",
        "        else:\n",
        "            decay.append(p)\n",
        "\n",
        "    return [\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0}]\n",
        "\n",
        "\n",
        "class WarmupCosineLR:\n",
        "    \"\"\"Warmup linear for warmup_steps, then cosine to min_lr. Step-based.\"\"\"\n",
        "    def __init__(self, optimizer, total_steps: int, warmup_steps: int, min_lr: float = 0.0):\n",
        "        self.optimizer = optimizer\n",
        "        self.total_steps = int(total_steps)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.min_lr = float(min_lr)\n",
        "        self.base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        t = self.step_num\n",
        "\n",
        "        for i, group in enumerate(self.optimizer.param_groups):\n",
        "            base = self.base_lrs[i]\n",
        "            if t <= self.warmup_steps and self.warmup_steps > 0:\n",
        "                lr = base * (t / self.warmup_steps)\n",
        "            else:\n",
        "                tt = min(t, self.total_steps)\n",
        "                denom = max(1, self.total_steps - self.warmup_steps)\n",
        "                progress = (tt - self.warmup_steps) / denom\n",
        "                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "                lr = self.min_lr + (base - self.min_lr) * cosine\n",
        "            group[\"lr\"] = lr\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\"step_num\": self.step_num}\n",
        "\n",
        "    def load_state_dict(self, d):\n",
        "        self.step_num = int(d.get(\"step_num\", 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vW315s-J9axB"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler,\n",
        "    device: str = \"cuda\",\n",
        "    scaler=None,\n",
        "    autocast_dtype: str = \"bf16\",\n",
        "    use_amp: bool = True,\n",
        "    grad_clip_norm: Optional[float] = 1.0,\n",
        "    label_smoothing: float = 0.1,\n",
        "    mixup_alpha: float = 0.0,\n",
        "    cutmix_alpha: float = 0.0,\n",
        "    mix_prob: float = 1.0,\n",
        "    num_classes: int = 100,\n",
        "    channels_last: bool = False,\n",
        "    print_every: int = 100,) -> Tuple[float, Dict[str, float], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Single-process train loop (no DDP, no EMA) + instrumentation.\n",
        "\n",
        "    Expects helpers already defined in your file:\n",
        "      - autocast_ctx\n",
        "      - apply_mixup_cutmix\n",
        "      - soft_target_cross_entropy\n",
        "      - accuracy_topk\n",
        "\n",
        "    Returns:\n",
        "      avg_loss, metrics(top1/top3/top5), extra(stats for logging)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Only use GradScaler for fp16 (as in your original logic)\n",
        "    use_scaler = (scaler is not None) and use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\")\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    c1 = c3 = c5 = 0.0\n",
        "\n",
        "    # instrumentation meters\n",
        "    grad_norm_sum = 0.0\n",
        "    grad_norm_count = 0\n",
        "    clip_steps = 0\n",
        "    overflow_steps = 0\n",
        "    nonfinite_loss_steps = 0\n",
        "\n",
        "    data_time_sum = 0.0\n",
        "    iter_time_sum = 0.0\n",
        "\n",
        "    # timing\n",
        "    t_epoch0 = time.time()\n",
        "    t_data = time.time()\n",
        "\n",
        "    for step, (images, targets) in enumerate(dataloader, start=1):\n",
        "        # data time\n",
        "        t0 = time.time()\n",
        "        data_time_sum += (t0 - t_data)\n",
        "\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        if channels_last:\n",
        "            images = images.contiguous(memory_format=torch.channels_last)\n",
        "\n",
        "        B = targets.size(0)\n",
        "\n",
        "        # mixup/cutmix => soft targets\n",
        "        images_aug, targets_soft = apply_mixup_cutmix(\n",
        "            images, targets,\n",
        "            num_classes=num_classes,\n",
        "            mixup_alpha=mixup_alpha,\n",
        "            cutmix_alpha=cutmix_alpha,\n",
        "            prob=mix_prob)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # forward under autocast\n",
        "        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n",
        "            logits = model(images_aug)  # [B, K]\n",
        "\n",
        "        # loss in fp32\n",
        "        if (mixup_alpha > 0.0) or (cutmix_alpha > 0.0):\n",
        "            # With mixup/cutmix, label smoothing is usually redundant.\n",
        "            loss = soft_target_cross_entropy(logits.float(), targets_soft)\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n",
        "\n",
        "        # guard non-finite loss\n",
        "        if not torch.isfinite(loss):\n",
        "            nonfinite_loss_steps += 1\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            # if fp16 scaler is used, count as overflow-like\n",
        "            if use_scaler:\n",
        "                overflow_steps += 1\n",
        "            # update timers\n",
        "            iter_time_sum += (time.time() - t0)\n",
        "            t_data = time.time()\n",
        "            continue\n",
        "\n",
        "        # backward + step (with grad norm + clipping + overflow detection)\n",
        "        if use_scaler:\n",
        "            scale_before = float(scaler.get_scale())\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # unscale before grad operations\n",
        "            scaler.unscale_(optimizer)\n",
        "\n",
        "            # compute grad norm + clip\n",
        "            if grad_clip_norm is not None:\n",
        "                gnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "                if float(gnorm) > float(grad_clip_norm):\n",
        "                    clip_steps += 1\n",
        "            else:\n",
        "                gnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), float(\"inf\"))\n",
        "\n",
        "            grad_norm_sum += float(gnorm)\n",
        "            grad_norm_count += 1\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scale_after = float(scaler.get_scale())\n",
        "            if scale_after < scale_before:\n",
        "                overflow_steps += 1\n",
        "\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "            if grad_clip_norm is not None:\n",
        "                gnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "                if float(gnorm) > float(grad_clip_norm):\n",
        "                    clip_steps += 1\n",
        "            else:\n",
        "                gnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), float(\"inf\"))\n",
        "\n",
        "            grad_norm_sum += float(gnorm)\n",
        "            grad_norm_count += 1\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # metrics\n",
        "        running_loss += float(loss.item()) * B\n",
        "        total += B\n",
        "\n",
        "        accs = accuracy_topk(\n",
        "            logits.detach(),\n",
        "            targets_soft if targets_soft.ndim == 2 else targets,\n",
        "            ks=(1, 3, 5),)\n",
        "\n",
        "        c1 += accs[1] * B / 100.0\n",
        "        c3 += accs[3] * B / 100.0\n",
        "        c5 += accs[5] * B / 100.0\n",
        "\n",
        "        # iter time\n",
        "        iter_time_sum += (time.time() - t0)\n",
        "\n",
        "        # logging\n",
        "        if print_every and (step % print_every == 0 or step == len(dataloader)):\n",
        "            dt = time.time() - t_epoch0\n",
        "            imgs_sec = total / max(dt, 1e-9)\n",
        "\n",
        "            gnorm_avg = grad_norm_sum / max(1, grad_norm_count)\n",
        "            clip_pct = 100.0 * clip_steps / max(1, grad_norm_count)\n",
        "            scale_now = float(scaler.get_scale()) if use_scaler else 1.0\n",
        "\n",
        "            print(\n",
        "                f\"[train step {step}/{len(dataloader)}] \"\n",
        "                f\"loss {running_loss/total:.4f} | \"\n",
        "                f\"top1 {100*c1/total:.2f}% | top3 {100*c3/total:.2f}% | top5 {100*c5/total:.2f}% | \"\n",
        "                f\"{imgs_sec:.1f} img/s | lr {optimizer.param_groups[0]['lr']:.2e} | \"\n",
        "                f\"gnorm {gnorm_avg:.3f} | clip {clip_pct:.1f}% | \"\n",
        "                f\"oflow {overflow_steps} | nonfinite {nonfinite_loss_steps} | scale {scale_now:.1f}\")\n",
        "\n",
        "        # reset for next loop\n",
        "        t_data = time.time()\n",
        "\n",
        "    avg_loss = running_loss / max(1, total)\n",
        "    metrics = {\n",
        "        \"top1\": 100.0 * c1 / max(1, total),\n",
        "        \"top3\": 100.0 * c3 / max(1, total),\n",
        "        \"top5\": 100.0 * c5 / max(1, total),}\n",
        "\n",
        "    extra = {\n",
        "        \"grad_norm_avg\": float(grad_norm_sum / max(1, grad_norm_count)),\n",
        "        \"clip_frac\": float(clip_steps / max(1, grad_norm_count)),\n",
        "        \"amp_overflow_steps\": float(overflow_steps),\n",
        "        \"nonfinite_loss_steps\": float(nonfinite_loss_steps),\n",
        "        \"scaler_scale\": float(scaler.get_scale()) if use_scaler else 1.0,\n",
        "        \"data_time_s_per_step\": float(data_time_sum / max(1, len(dataloader))),\n",
        "        \"iter_time_s_per_step\": float(iter_time_sum / max(1, len(dataloader))),}\n",
        "\n",
        "    return avg_loss, metrics, extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "O7MaseFT_Rqd"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader,\n",
        "    device: str = \"cuda\",\n",
        "    autocast_dtype: str = \"bf16\",\n",
        "    use_amp: bool = True,\n",
        "    label_smoothing: float = 0.0,\n",
        "    channels_last: bool = False) -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Single-process evaluation loop (no DDP, no EMA).\n",
        "\n",
        "    Expects helpers already defined in your file:\n",
        "      - autocast_ctx\n",
        "      - accuracy_topk\n",
        "    \"\"\"\n",
        "    model.eval().to(device)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    c1 = c3 = c5 = 0.0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        if channels_last:\n",
        "            images = images.contiguous(memory_format=torch.channels_last)\n",
        "\n",
        "        B = targets.size(0)\n",
        "\n",
        "        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n",
        "            logits = model(images)\n",
        "\n",
        "        loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n",
        "\n",
        "        running_loss += loss.item() * B\n",
        "        total += B\n",
        "\n",
        "        accs = accuracy_topk(logits, targets, ks=(1, 3, 5))\n",
        "        c1 += accs[1] * B / 100.0\n",
        "        c3 += accs[3] * B / 100.0\n",
        "        c5 += accs[5] * B / 100.0\n",
        "\n",
        "    avg_loss = running_loss / max(1, total)\n",
        "    metrics = {\n",
        "        \"top1\": 100.0 * c1 / max(1, total),\n",
        "        \"top3\": 100.0 * c3 / max(1, total),\n",
        "        \"top5\": 100.0 * c5 / max(1, total),}\n",
        "\n",
        "    return avg_loss, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pWPxUzjx_Ehy"
      },
      "outputs": [],
      "source": [
        "def _bytes_to_gib(x: int) -> float:\n",
        "    return float(x) / (1024 ** 3)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _cuda_mem_str() -> str:\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"mem n/a\"\n",
        "    alloc = _bytes_to_gib(torch.cuda.max_memory_allocated())\n",
        "    reserv = _bytes_to_gib(torch.cuda.max_memory_reserved())\n",
        "    return f\"mem_peak alloc {alloc:.2f} GiB | reserved {reserv:.2f} GiB\"\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader,\n",
        "    epochs: int,\n",
        "    val_loader=None,\n",
        "    device: str = \"cuda\",\n",
        "    lr: float = 5e-4,\n",
        "    weight_decay: float = 0.05,\n",
        "    autocast_dtype: str = \"fp16\",\n",
        "    use_amp: bool = True,\n",
        "    grad_clip_norm: float | None = 1.0,\n",
        "    warmup_ratio: float = 0.05,\n",
        "    min_lr: float = 0.0,\n",
        "    label_smoothing: float = 0.1,\n",
        "    print_every: int = 100,\n",
        "    save_path: str = \"best_model.pt\",\n",
        "    last_path: str = \"last_model.pt\",\n",
        "    resume_path: str | None = None,\n",
        "    mixup_alpha: float = 0.0,\n",
        "    cutmix_alpha: float = 0.0,\n",
        "    mix_prob: float = 1.0,\n",
        "    num_classes: int = 100,\n",
        "    channels_last: bool = False,\n",
        "    early_stop: bool = True,\n",
        "    early_stop_metric: str = \"top1\",\n",
        "    early_stop_patience: int = 10,\n",
        "    early_stop_min_delta: float = 0.0,\n",
        "    early_stop_require_monotonic: bool = False,) -> Tuple[Dict[str, list], nn.Module]:\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    param_groups = build_param_groups_no_wd(model, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "\n",
        "    # Scheduler warmup + cosine (step-based)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "    scheduler = WarmupCosineLR(\n",
        "        optimizer,\n",
        "        total_steps=total_steps,\n",
        "        warmup_steps=warmup_steps,\n",
        "        min_lr=min_lr)\n",
        "\n",
        "    # AMP scaler\n",
        "    scaler = None\n",
        "    if use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\"):\n",
        "        scaler = make_grad_scaler(device=device, enabled=True)\n",
        "\n",
        "    # Resume\n",
        "    start_epoch = 0\n",
        "    best_val_top1 = -float(\"inf\")\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "\n",
        "    if resume_path is not None:\n",
        "        ckpt = load_checkpoint(\n",
        "            resume_path, model,\n",
        "            optimizer=optimizer, scheduler=scheduler, scaler=scaler,\n",
        "            map_location=device,\n",
        "            strict=True)\n",
        "\n",
        "        start_epoch = int(ckpt.get(\"epoch\", 0))\n",
        "        best_val_top1 = float(ckpt.get(\"best_top1\", best_val_top1))\n",
        "        extra = ckpt.get(\"extra\", {}) or {}\n",
        "        best_val_loss = float(extra.get(\"best_val_loss\", best_val_loss))\n",
        "        best_epoch = int(extra.get(\"best_epoch\", best_epoch))\n",
        "        print(f\"Resumed from {resume_path} at epoch {start_epoch} | best_top1 {best_val_top1:.2f}% | best_loss {best_val_loss:.4f}\")\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"train_top1\": [], \"train_top3\": [], \"train_top5\": [],\n",
        "        \"val_loss\": [], \"val_top1\": [], \"val_top3\": [], \"val_top5\": [],\n",
        "        \"lr\": [],\n",
        "        \"train_grad_norm\": [], \"train_clip_frac\": [], \"train_amp_overflows\": [],\n",
        "        \"train_nonfinite_loss_steps\": [], \"train_scaler_scale\": [],\n",
        "        \"train_mem_alloc_gib\": [], \"train_mem_res_gib\": [],\n",
        "        \"val_mem_alloc_gib\": [], \"val_mem_res_gib\": []}\n",
        "\n",
        "    metric = early_stop_metric.lower()\n",
        "    assert metric in (\"top1\", \"loss\")\n",
        "    mode = \"max\" if metric == \"top1\" else \"min\"\n",
        "    best_metric = best_val_top1 if metric == \"top1\" else best_val_loss\n",
        "    patience = int(early_stop_patience)\n",
        "    bad_epochs = 0\n",
        "    last_vals: list[float] = []\n",
        "\n",
        "    def _is_improvement(curr: float, best: float) -> bool:\n",
        "        d = float(early_stop_min_delta)\n",
        "        return (curr > (best + d)) if mode == \"max\" else (curr < (best - d))\n",
        "\n",
        "    def _degradation_monotonic(vals: list[float]) -> bool:\n",
        "        if not early_stop_require_monotonic or len(vals) < 2:\n",
        "            return True\n",
        "        if mode == \"max\":\n",
        "            return all(vals[i] >= vals[i + 1] for i in range(len(vals) - 1))\n",
        "        else:\n",
        "            return all(vals[i] <= vals[i + 1] for i in range(len(vals) - 1))\n",
        "\n",
        "\n",
        "    first_batch = next(iter(train_loader))\n",
        "    x0, y0 = first_batch[0], first_batch[1]\n",
        "    bs0 = x0.size(0)\n",
        "    img_shape = tuple(x0.shape)\n",
        "\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "\n",
        "    print(\"=== Run config ===\")\n",
        "    print(f\"device={device} | amp={use_amp} | autocast_dtype={autocast_dtype} | channels_last={channels_last}\")\n",
        "    print(f\"epochs={epochs} | steps/epoch={len(train_loader)} | total_steps={total_steps} | warmup_steps={warmup_steps}\")\n",
        "    print(f\"batch_size={bs0} | input_shape={img_shape} | num_classes={num_classes}\")\n",
        "    print(f\"opt=AdamW | lr={lr} | wd={weight_decay} | grad_clip_norm={grad_clip_norm}\")\n",
        "    print(f\"aug: mix_prob={mix_prob} | mixup_alpha={mixup_alpha} | cutmix_alpha={cutmix_alpha} | label_smoothing={label_smoothing}\")\n",
        "    print(\"==================\")\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch + 1, epochs + 1):\n",
        "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
        "        t_epoch = time.time()\n",
        "\n",
        "        # reshuffle if sampler supports it\n",
        "        if hasattr(train_loader, \"sampler\") and hasattr(train_loader.sampler, \"set_epoch\"):\n",
        "            train_loader.sampler.set_epoch(epoch)\n",
        "        if val_loader is not None and hasattr(val_loader, \"sampler\") and hasattr(val_loader.sampler, \"set_epoch\"):\n",
        "            val_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "        # reset CUDA peak stats (train)\n",
        "        if torch.cuda.is_available() and (\"cuda\" in device):\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        # Train\n",
        "        tr_loss, tr_m, tr_extra = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            device=device,\n",
        "            scaler=scaler,\n",
        "            autocast_dtype=autocast_dtype,\n",
        "            use_amp=use_amp,\n",
        "            grad_clip_norm=grad_clip_norm,\n",
        "            label_smoothing=label_smoothing,\n",
        "            mixup_alpha=mixup_alpha,\n",
        "            cutmix_alpha=cutmix_alpha,\n",
        "            mix_prob=mix_prob,\n",
        "            num_classes=num_classes,\n",
        "            channels_last=channels_last,\n",
        "            print_every=print_every,)\n",
        "\n",
        "        # capture train peak VRAM\n",
        "        train_alloc_gib = None\n",
        "        train_res_gib = None\n",
        "        if torch.cuda.is_available() and (\"cuda\" in device):\n",
        "            train_alloc_gib = _bytes_to_gib(torch.cuda.max_memory_allocated())\n",
        "            train_res_gib = _bytes_to_gib(torch.cuda.max_memory_reserved())\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"train_top1\"].append(tr_m[\"top1\"])\n",
        "        history[\"train_top3\"].append(tr_m[\"top3\"])\n",
        "        history[\"train_top5\"].append(tr_m[\"top5\"])\n",
        "        history[\"lr\"].append(float(optimizer.param_groups[0][\"lr\"]))\n",
        "\n",
        "        history[\"train_grad_norm\"].append(float(tr_extra[\"grad_norm_avg\"]))\n",
        "        history[\"train_clip_frac\"].append(float(tr_extra[\"clip_frac\"]))\n",
        "        history[\"train_amp_overflows\"].append(float(tr_extra[\"amp_overflow_steps\"]))\n",
        "        history[\"train_nonfinite_loss_steps\"].append(float(tr_extra[\"nonfinite_loss_steps\"]))\n",
        "        history[\"train_scaler_scale\"].append(float(tr_extra[\"scaler_scale\"]))\n",
        "\n",
        "        history[\"train_mem_alloc_gib\"].append(float(train_alloc_gib) if train_alloc_gib is not None else float(\"nan\"))\n",
        "        history[\"train_mem_res_gib\"].append(float(train_res_gib) if train_res_gib is not None else float(\"nan\"))\n",
        "\n",
        "        print(\n",
        "            f\"[Train] loss {tr_loss:.4f} | top1 {tr_m['top1']:.2f}% | top3 {tr_m['top3']:.2f}% | top5 {tr_m['top5']:.2f}% | \"\n",
        "            f\"lr {optimizer.param_groups[0]['lr']:.2e} | \"\n",
        "            f\"grad_norm {tr_extra['grad_norm_avg']:.3f} | clip {100*tr_extra['clip_frac']:.1f}% | \"\n",
        "            f\"amp_overflows {int(tr_extra['amp_overflow_steps'])} | \"\n",
        "            f\"nonfinite_loss {int(tr_extra['nonfinite_loss_steps'])} | \"\n",
        "            f\"scale {tr_extra['scaler_scale']:.1f}\")\n",
        "\n",
        "        if torch.cuda.is_available() and (\"cuda\" in device):\n",
        "            print(f\"[Train] {_cuda_mem_str()}\")\n",
        "\n",
        "        # Save \"last\" checkpoint every epoch\n",
        "        save_checkpoint(\n",
        "            last_path, model, optimizer, scheduler, scaler,\n",
        "            epoch=epoch, best_top1=best_val_top1,\n",
        "            extra={\n",
        "                \"autocast_dtype\": autocast_dtype,\n",
        "                \"use_amp\": use_amp,\n",
        "                \"best_val_loss\": best_val_loss,\n",
        "                \"best_epoch\": best_epoch,\n",
        "                \"early_stop_metric\": metric,\n",
        "                \"early_stop_patience\": patience,\n",
        "                \"early_stop_min_delta\": float(early_stop_min_delta),},)\n",
        "\n",
        "        stop_now = False\n",
        "\n",
        "        # Val\n",
        "        if val_loader is not None:\n",
        "            # reset CUDA peak stats (val) to log val peak separately\n",
        "            if torch.cuda.is_available() and (\"cuda\" in device):\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            va_loss, va_m = evaluate_one_epoch(\n",
        "                model=model,\n",
        "                dataloader=val_loader,\n",
        "                device=device,\n",
        "                autocast_dtype=autocast_dtype,\n",
        "                use_amp=use_amp,\n",
        "                label_smoothing=0.0,\n",
        "                channels_last=channels_last,)\n",
        "\n",
        "            val_alloc_gib = None\n",
        "            val_res_gib = None\n",
        "            if torch.cuda.is_available() and (\"cuda\" in device):\n",
        "                val_alloc_gib = _bytes_to_gib(torch.cuda.max_memory_allocated())\n",
        "                val_res_gib = _bytes_to_gib(torch.cuda.max_memory_reserved())\n",
        "\n",
        "            history[\"val_loss\"].append(va_loss)\n",
        "            history[\"val_top1\"].append(va_m[\"top1\"])\n",
        "            history[\"val_top3\"].append(va_m[\"top3\"])\n",
        "            history[\"val_top5\"].append(va_m[\"top5\"])\n",
        "            history[\"val_mem_alloc_gib\"].append(float(val_alloc_gib) if val_alloc_gib is not None else float(\"nan\"))\n",
        "            history[\"val_mem_res_gib\"].append(float(val_res_gib) if val_res_gib is not None else float(\"nan\"))\n",
        "\n",
        "            print(\n",
        "                f\"[Val]   loss {va_loss:.4f} | top1 {va_m['top1']:.2f}% | top3 {va_m['top3']:.2f}% | top5 {va_m['top5']:.2f}%\")\n",
        "\n",
        "            if torch.cuda.is_available() and (\"cuda\" in device):\n",
        "                print(f\"[Val]   {_cuda_mem_str()}\")\n",
        "\n",
        "            # Best checkpoint by val_top1\n",
        "            if va_m[\"top1\"] > best_val_top1:\n",
        "                best_val_top1 = float(va_m[\"top1\"])\n",
        "                if va_loss < best_val_loss:\n",
        "                    best_val_loss = float(va_loss)\n",
        "                    best_epoch = int(epoch)\n",
        "\n",
        "                save_checkpoint(\n",
        "                    save_path, model, optimizer, scheduler, scaler,\n",
        "                    epoch=epoch, best_top1=best_val_top1,\n",
        "                    extra={\n",
        "                        \"autocast_dtype\": autocast_dtype,\n",
        "                        \"use_amp\": use_amp,\n",
        "                        \"best_val_loss\": best_val_loss,\n",
        "                        \"best_epoch\": best_epoch, },)\n",
        "\n",
        "                print(f\"Best saved to {save_path} (val top1 {best_val_top1:.2f}%)\")\n",
        "\n",
        "            # Early stop on chosen metric\n",
        "            if early_stop:\n",
        "                curr_metric = float(va_m[\"top1\"]) if metric == \"top1\" else float(va_loss)\n",
        "\n",
        "                last_vals.append(curr_metric)\n",
        "                if len(last_vals) > patience:\n",
        "                    last_vals = last_vals[-patience:]\n",
        "\n",
        "                if _is_improvement(curr_metric, best_metric):\n",
        "                    best_metric = curr_metric\n",
        "                    bad_epochs = 0\n",
        "                else:\n",
        "                    bad_epochs += 1\n",
        "\n",
        "                if bad_epochs >= patience and _degradation_monotonic(last_vals):\n",
        "                    print(f\"Early-stop: no improvement on val_{metric} for {patience} epochs.\")\n",
        "                    stop_now = True\n",
        "\n",
        "        if stop_now:\n",
        "            break\n",
        "\n",
        "        dt = time.time() - t_epoch\n",
        "        print(f\"Epoch time: {dt/60:.2f} min\")\n",
        "\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIv5mdnqAX74",
        "outputId": "e57b1ae7-5373-4fc7-a294-cac78c897253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CUDA] allocated=0.0 MB | reserved(cache)=0.0 MB\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "def free_all_cuda(*names, verbose=True, globals_dict=None, locals_dict=None):\n",
        "    \"\"\"\n",
        "    Borra variables por nombre (strings) de globals/locals para evitar referencias colgadas en notebooks.\n",
        "    \"\"\"\n",
        "    if globals_dict is None: globals_dict = globals()\n",
        "    if locals_dict is None:  locals_dict  = locals()\n",
        "\n",
        "    for n in names:\n",
        "        if n in locals_dict:\n",
        "            del locals_dict[n]\n",
        "        if n in globals_dict:\n",
        "            del globals_dict[n]\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "    if verbose and torch.cuda.is_available():\n",
        "        alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "        res   = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"[CUDA] allocated={alloc:.1f} MB | reserved(cache)={res:.1f} MB\")\n",
        "\n",
        "free_all_cuda(\"model\", \"optimizer\", \"scaler\", \"scheduler\", \"batch\", \"loss\", \"outputs\", \"logits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5SEa3dGweEk"
      },
      "outputs": [],
      "source": [
        "def cifar32_stages_t4_tinyplus(drop_path=0.08):\n",
        "    # resoluciones: 64 -> 32 -> 16 -> 8\n",
        "    return [\n",
        "        StageCfg(dim=80,  depth=2, num_heads=2,  grid_size=4, outlook_heads=2,  drop_path=drop_path),\n",
        "        StageCfg(dim=160, depth=3, num_heads=5,  grid_size=4, outlook_heads=5,  drop_path=drop_path),\n",
        "        StageCfg(dim=320, depth=4, num_heads=10, grid_size=2, outlook_heads=10, drop_path=drop_path),\n",
        "        StageCfg(dim=448, depth=2, num_heads=8,  grid_size=1, outlook_heads=8,  drop_path=drop_path)]\n",
        "\n",
        "stages = cifar32_stages_t4_tinyplus(drop_path=0.10)\n",
        "\n",
        "model = MaxOutNet(\n",
        "    num_classes=100,\n",
        "    stages=stages,\n",
        "    stem_dim=64,\n",
        "    dpr_max=0.12)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnV5yyWJwfLL",
        "outputId": "c8ea5a80-44ac-445f-a729-20ebea070c6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 32,974,583\n"
          ]
        }
      ],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params = count_trainable_parameters(model)\n",
        "print(f\"Trainable parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWflG0_8_doZ",
        "outputId": "77410c11-bfba-4ec9-c3a7-fde4d4f96cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Run config ===\n",
            "device=cuda | amp=True | autocast_dtype=fp16 | channels_last=True\n",
            "epochs=50 | steps/epoch=704 | total_steps=35200 | warmup_steps=1760\n",
            "batch_size=64 | input_shape=(64, 3, 32, 32) | num_classes=100\n",
            "opt=AdamW | lr=0.0005 | wd=0.05 | grad_clip_norm=1.0\n",
            "aug: mix_prob=0.5 | mixup_alpha=0.0 | cutmix_alpha=1.0 | label_smoothing=0.0\n",
            "==================\n",
            "\n",
            "=== Epoch 1/50 ===\n",
            "[train step 100/704] loss 4.5565 | top1 2.69% | top3 6.70% | top5 10.14% | 114.2 img/s | lr 2.84e-05 | gnorm 8.440 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 4.4513 | top1 3.63% | top3 9.41% | top5 13.98% | 143.1 img/s | lr 5.68e-05 | gnorm 7.938 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 4.3773 | top1 4.54% | top3 11.35% | top5 16.70% | 156.3 img/s | lr 8.52e-05 | gnorm 7.437 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 4.3407 | top1 5.00% | top3 12.44% | top5 18.14% | 163.5 img/s | lr 1.14e-04 | gnorm 6.877 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 4.3023 | top1 5.52% | top3 13.33% | top5 19.48% | 168.4 img/s | lr 1.42e-04 | gnorm 6.386 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 4.2710 | top1 5.94% | top3 14.28% | top5 20.71% | 171.7 img/s | lr 1.70e-04 | gnorm 5.993 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 4.2303 | top1 6.54% | top3 15.45% | top5 22.10% | 174.1 img/s | lr 1.99e-04 | gnorm 5.699 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 4.2280 | top1 6.59% | top3 15.52% | top5 22.18% | 160.1 img/s | lr 2.00e-04 | gnorm 5.697 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 4.2280 | top1 6.59% | top3 15.52% | top5 22.18% | lr 2.00e-04 | grad_norm 5.697 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 5.32 GiB | reserved 5.35 GiB\n",
            "[Val]   loss 3.8110 | top1 11.12% | top3 24.82% | top5 34.28%\n",
            "[Val]   mem_peak alloc 0.85 GiB | reserved 1.14 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 11.12%)\n",
            "Epoch time: 4.97 min\n",
            "\n",
            "=== Epoch 2/50 ===\n",
            "[train step 100/704] loss 3.9532 | top1 10.73% | top3 23.66% | top5 31.81% | 186.5 img/s | lr 2.28e-04 | gnorm 3.746 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 3.9212 | top1 11.30% | top3 24.70% | top5 33.21% | 188.7 img/s | lr 2.57e-04 | gnorm 3.655 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 3.9000 | top1 11.62% | top3 25.19% | top5 33.87% | 189.5 img/s | lr 2.85e-04 | gnorm 3.567 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 3.8683 | top1 12.02% | top3 25.91% | top5 34.94% | 189.4 img/s | lr 3.14e-04 | gnorm 3.509 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 3.8485 | top1 12.29% | top3 26.32% | top5 35.43% | 189.7 img/s | lr 3.42e-04 | gnorm 3.431 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 3.8269 | top1 12.67% | top3 26.97% | top5 36.17% | 189.8 img/s | lr 3.70e-04 | gnorm 3.366 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 3.7931 | top1 13.25% | top3 27.88% | top5 37.23% | 189.8 img/s | lr 3.99e-04 | gnorm 3.334 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 3.7916 | top1 13.28% | top3 27.93% | top5 37.28% | 189.8 img/s | lr 4.00e-04 | gnorm 3.336 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 3.7916 | top1 13.28% | top3 27.93% | top5 37.28% | lr 4.00e-04 | grad_norm 3.336 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 3.4123 | top1 17.76% | top3 35.42% | top5 46.20%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 17.76%)\n",
            "Epoch time: 4.26 min\n",
            "\n",
            "=== Epoch 3/50 ===\n",
            "[train step 100/704] loss 3.5191 | top1 18.16% | top3 34.94% | top5 44.80% | 187.9 img/s | lr 4.28e-04 | gnorm 3.166 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 3.5388 | top1 17.93% | top3 34.73% | top5 44.30% | 189.0 img/s | lr 4.57e-04 | gnorm 3.090 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 3.5600 | top1 17.62% | top3 34.15% | top5 43.91% | 189.1 img/s | lr 4.85e-04 | gnorm 2.983 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 3.5451 | top1 17.81% | top3 34.65% | top5 44.43% | 189.5 img/s | lr 5.00e-04 | gnorm 2.956 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 3.5178 | top1 18.30% | top3 35.42% | top5 45.38% | 189.7 img/s | lr 5.00e-04 | gnorm 2.953 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 3.4994 | top1 18.82% | top3 36.03% | top5 45.94% | 189.7 img/s | lr 5.00e-04 | gnorm 2.921 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 3.4872 | top1 19.14% | top3 36.48% | top5 46.46% | 189.9 img/s | lr 5.00e-04 | gnorm 2.885 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 3.4855 | top1 19.17% | top3 36.54% | top5 46.52% | 189.8 img/s | lr 5.00e-04 | gnorm 2.892 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 3.4855 | top1 19.17% | top3 36.54% | top5 46.52% | lr 5.00e-04 | grad_norm 2.892 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 3.0234 | top1 23.82% | top3 44.98% | top5 55.96%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 23.82%)\n",
            "Epoch time: 4.32 min\n",
            "\n",
            "=== Epoch 4/50 ===\n",
            "[train step 100/704] loss 3.3073 | top1 22.33% | top3 41.22% | top5 52.00% | 188.2 img/s | lr 5.00e-04 | gnorm 2.805 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 200/704] loss 3.2773 | top1 23.23% | top3 42.40% | top5 52.83% | 189.4 img/s | lr 5.00e-04 | gnorm 2.765 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 3.2479 | top1 24.09% | top3 43.22% | top5 53.67% | 190.1 img/s | lr 5.00e-04 | gnorm 2.790 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 3.2202 | top1 24.57% | top3 43.77% | top5 54.30% | 190.1 img/s | lr 4.99e-04 | gnorm 2.812 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 500/704] loss 3.2296 | top1 24.60% | top3 43.76% | top5 54.12% | 190.2 img/s | lr 4.99e-04 | gnorm 2.785 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 3.2265 | top1 24.86% | top3 44.07% | top5 54.41% | 190.4 img/s | lr 4.99e-04 | gnorm 2.766 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 3.2110 | top1 25.25% | top3 44.47% | top5 54.79% | 190.3 img/s | lr 4.99e-04 | gnorm 2.752 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 3.2106 | top1 25.24% | top3 44.48% | top5 54.82% | 190.2 img/s | lr 4.99e-04 | gnorm 2.759 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 3.2106 | top1 25.24% | top3 44.48% | top5 54.82% | lr 4.99e-04 | grad_norm 2.759 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 2.6075 | top1 33.46% | top3 55.16% | top5 65.14%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 33.46%)\n",
            "Epoch time: 4.20 min\n",
            "\n",
            "=== Epoch 5/50 ===\n",
            "[train step 100/704] loss 3.0878 | top1 29.12% | top3 48.97% | top5 59.03% | 188.9 img/s | lr 4.99e-04 | gnorm 2.683 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 200/704] loss 3.0539 | top1 28.95% | top3 49.16% | top5 59.20% | 189.4 img/s | lr 4.98e-04 | gnorm 2.716 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 3.0518 | top1 28.64% | top3 48.84% | top5 58.91% | 189.9 img/s | lr 4.98e-04 | gnorm 2.724 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 3.0222 | top1 29.04% | top3 49.53% | top5 59.52% | 190.0 img/s | lr 4.98e-04 | gnorm 2.728 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 500/704] loss 3.0345 | top1 28.91% | top3 49.42% | top5 59.30% | 189.9 img/s | lr 4.97e-04 | gnorm 2.710 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 3.0059 | top1 29.45% | top3 50.06% | top5 60.07% | 190.0 img/s | lr 4.97e-04 | gnorm 2.718 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 2.9808 | top1 30.03% | top3 50.72% | top5 60.72% | 190.1 img/s | lr 4.97e-04 | gnorm 2.734 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 2.9807 | top1 30.06% | top3 50.72% | top5 60.70% | 190.1 img/s | lr 4.97e-04 | gnorm 2.738 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 2.9807 | top1 30.06% | top3 50.72% | top5 60.70% | lr 4.97e-04 | grad_norm 2.738 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 2.4248 | top1 35.86% | top3 59.18% | top5 69.58%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 35.86%)\n",
            "Epoch time: 4.30 min\n",
            "\n",
            "=== Epoch 6/50 ===\n",
            "[train step 100/704] loss 2.8719 | top1 32.09% | top3 52.91% | top5 63.09% | 187.8 img/s | lr 4.96e-04 | gnorm 2.718 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 200/704] loss 2.8851 | top1 32.02% | top3 52.39% | top5 62.15% | 189.5 img/s | lr 4.96e-04 | gnorm 2.690 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 2.8545 | top1 32.31% | top3 53.04% | top5 62.70% | 189.9 img/s | lr 4.95e-04 | gnorm 2.694 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 2.8324 | top1 32.96% | top3 53.75% | top5 63.50% | 189.9 img/s | lr 4.95e-04 | gnorm 2.693 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 500/704] loss 2.8354 | top1 33.13% | top3 54.00% | top5 63.78% | 190.1 img/s | lr 4.94e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 2.8114 | top1 33.61% | top3 54.53% | top5 64.29% | 190.1 img/s | lr 4.94e-04 | gnorm inf | clip 100.0% | oflow 2 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.7821 | top1 34.08% | top3 55.11% | top5 64.87% | 190.2 img/s | lr 4.93e-04 | gnorm inf | clip 100.0% | oflow 2 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.7819 | top1 34.08% | top3 55.13% | top5 64.87% | 190.1 img/s | lr 4.93e-04 | gnorm inf | clip 100.0% | oflow 2 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.7819 | top1 34.08% | top3 55.13% | top5 64.87% | lr 4.93e-04 | grad_norm inf | clip 100.0% | amp_overflows 2 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 2.2327 | top1 40.80% | top3 64.24% | top5 73.92%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 40.80%)\n",
            "Epoch time: 4.37 min\n",
            "\n",
            "=== Epoch 7/50 ===\n",
            "[train step 100/704] loss 2.6762 | top1 37.11% | top3 58.39% | top5 68.06% | 188.4 img/s | lr 4.93e-04 | gnorm 2.751 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.6686 | top1 37.52% | top3 58.92% | top5 68.11% | 189.8 img/s | lr 4.92e-04 | gnorm 2.754 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.6129 | top1 38.20% | top3 59.73% | top5 68.85% | 190.2 img/s | lr 4.92e-04 | gnorm 2.797 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.5980 | top1 38.38% | top3 59.99% | top5 69.19% | 190.4 img/s | lr 4.91e-04 | gnorm 2.798 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.5962 | top1 38.55% | top3 60.06% | top5 69.24% | 190.6 img/s | lr 4.90e-04 | gnorm 2.791 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.6251 | top1 38.36% | top3 59.67% | top5 68.88% | 190.7 img/s | lr 4.90e-04 | gnorm 2.758 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.6249 | top1 38.41% | top3 59.67% | top5 68.82% | 190.8 img/s | lr 4.89e-04 | gnorm 2.752 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.6246 | top1 38.44% | top3 59.66% | top5 68.82% | 190.8 img/s | lr 4.89e-04 | gnorm 2.756 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.6246 | top1 38.44% | top3 59.66% | top5 68.82% | lr 4.89e-04 | grad_norm 2.756 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 2.2024 | top1 42.00% | top3 64.34% | top5 74.18%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 42.00%)\n",
            "Epoch time: 4.35 min\n",
            "\n",
            "=== Epoch 8/50 ===\n",
            "[train step 100/704] loss 2.6920 | top1 37.16% | top3 58.80% | top5 68.28% | 187.5 img/s | lr 4.88e-04 | gnorm 2.646 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.6653 | top1 38.16% | top3 59.35% | top5 68.70% | 189.7 img/s | lr 4.88e-04 | gnorm 2.664 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.6468 | top1 38.26% | top3 59.30% | top5 68.48% | 190.2 img/s | lr 4.87e-04 | gnorm 2.674 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.6450 | top1 38.52% | top3 59.50% | top5 68.57% | 190.4 img/s | lr 4.86e-04 | gnorm 2.688 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.6299 | top1 38.89% | top3 59.90% | top5 68.93% | 190.6 img/s | lr 4.85e-04 | gnorm 2.684 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.5983 | top1 39.52% | top3 60.48% | top5 69.53% | 190.7 img/s | lr 4.85e-04 | gnorm 2.694 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.6054 | top1 39.41% | top3 60.28% | top5 69.42% | 190.8 img/s | lr 4.84e-04 | gnorm 2.683 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.6037 | top1 39.46% | top3 60.32% | top5 69.47% | 190.7 img/s | lr 4.84e-04 | gnorm 2.689 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.6037 | top1 39.46% | top3 60.32% | top5 69.47% | lr 4.84e-04 | grad_norm 2.689 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 2.0351 | top1 45.96% | top3 68.12% | top5 77.32%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 45.96%)\n",
            "Epoch time: 4.25 min\n",
            "\n",
            "=== Epoch 9/50 ===\n",
            "[train step 100/704] loss 2.5723 | top1 39.80% | top3 61.23% | top5 70.48% | 188.3 img/s | lr 4.83e-04 | gnorm 2.641 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.4698 | top1 41.84% | top3 63.30% | top5 72.16% | 190.0 img/s | lr 4.82e-04 | gnorm 2.699 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.4597 | top1 42.19% | top3 63.51% | top5 72.39% | 190.5 img/s | lr 4.81e-04 | gnorm 2.703 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.4519 | top1 42.55% | top3 63.81% | top5 72.56% | 190.7 img/s | lr 4.80e-04 | gnorm 2.693 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.4416 | top1 42.70% | top3 63.95% | top5 72.67% | 190.7 img/s | lr 4.79e-04 | gnorm 2.698 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 2.4630 | top1 42.34% | top3 63.47% | top5 72.27% | 190.8 img/s | lr 4.78e-04 | gnorm 2.696 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 2.4601 | top1 42.47% | top3 63.50% | top5 72.30% | 190.9 img/s | lr 4.77e-04 | gnorm 2.695 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 2.4578 | top1 42.49% | top3 63.53% | top5 72.34% | 190.8 img/s | lr 4.77e-04 | gnorm 2.699 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 2.4578 | top1 42.49% | top3 63.53% | top5 72.34% | lr 4.77e-04 | grad_norm 2.699 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.8252 | top1 50.94% | top3 72.72% | top5 81.50%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 50.94%)\n",
            "Epoch time: 4.34 min\n",
            "\n",
            "=== Epoch 10/50 ===\n",
            "[train step 100/704] loss 2.2879 | top1 45.66% | top3 67.19% | top5 75.69% | 189.0 img/s | lr 4.76e-04 | gnorm 2.704 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 200/704] loss 2.3986 | top1 44.34% | top3 64.96% | top5 73.84% | 190.3 img/s | lr 4.75e-04 | gnorm 2.659 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 2.4137 | top1 43.96% | top3 64.86% | top5 73.58% | 190.6 img/s | lr 4.74e-04 | gnorm 2.689 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 2.4079 | top1 43.98% | top3 64.82% | top5 73.49% | 190.6 img/s | lr 4.73e-04 | gnorm 2.688 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 500/704] loss 2.4277 | top1 43.70% | top3 64.36% | top5 73.06% | 190.8 img/s | lr 4.72e-04 | gnorm 2.675 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 2.4249 | top1 43.77% | top3 64.55% | top5 73.29% | 190.8 img/s | lr 4.71e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.4343 | top1 43.72% | top3 64.42% | top5 73.19% | 190.9 img/s | lr 4.70e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.4329 | top1 43.73% | top3 64.46% | top5 73.24% | 190.8 img/s | lr 4.70e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.4329 | top1 43.73% | top3 64.46% | top5 73.24% | lr 4.70e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.8055 | top1 52.00% | top3 73.74% | top5 82.22%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 52.00%)\n",
            "Epoch time: 4.45 min\n",
            "\n",
            "=== Epoch 11/50 ===\n",
            "[train step 100/704] loss 2.3858 | top1 45.00% | top3 65.83% | top5 74.48% | 188.2 img/s | lr 4.69e-04 | gnorm 2.603 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.3019 | top1 46.71% | top3 67.73% | top5 76.11% | 190.1 img/s | lr 4.68e-04 | gnorm 2.672 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.2454 | top1 47.62% | top3 68.40% | top5 76.60% | 190.7 img/s | lr 4.66e-04 | gnorm 2.718 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.2754 | top1 47.29% | top3 67.85% | top5 76.07% | 190.6 img/s | lr 4.65e-04 | gnorm 2.705 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.2849 | top1 47.02% | top3 67.58% | top5 75.93% | 190.9 img/s | lr 4.64e-04 | gnorm 2.702 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.2907 | top1 46.76% | top3 67.30% | top5 75.56% | 190.9 img/s | lr 4.63e-04 | gnorm 2.698 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.2733 | top1 46.93% | top3 67.57% | top5 75.79% | 190.9 img/s | lr 4.62e-04 | gnorm 2.708 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.2702 | top1 47.00% | top3 67.63% | top5 75.85% | 190.9 img/s | lr 4.62e-04 | gnorm 2.715 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.2702 | top1 47.00% | top3 67.63% | top5 75.85% | lr 4.62e-04 | grad_norm 2.715 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.6847 | top1 55.12% | top3 75.50% | top5 82.84%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 55.12%)\n",
            "Epoch time: 4.32 min\n",
            "\n",
            "=== Epoch 12/50 ===\n",
            "[train step 100/704] loss 2.1678 | top1 49.19% | top3 69.69% | top5 77.92% | 188.7 img/s | lr 4.60e-04 | gnorm 2.733 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.1616 | top1 49.05% | top3 69.73% | top5 77.68% | 190.2 img/s | lr 4.59e-04 | gnorm 2.749 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.2087 | top1 48.20% | top3 69.01% | top5 76.93% | 190.5 img/s | lr 4.58e-04 | gnorm 2.730 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.2148 | top1 48.45% | top3 69.11% | top5 77.02% | 190.7 img/s | lr 4.56e-04 | gnorm 2.731 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.2270 | top1 48.18% | top3 68.78% | top5 76.81% | 190.9 img/s | lr 4.55e-04 | gnorm 2.729 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.2311 | top1 48.13% | top3 68.60% | top5 76.66% | 190.9 img/s | lr 4.54e-04 | gnorm 2.727 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.2208 | top1 48.31% | top3 68.73% | top5 76.82% | 191.0 img/s | lr 4.52e-04 | gnorm 2.739 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.2202 | top1 48.34% | top3 68.74% | top5 76.84% | 190.9 img/s | lr 4.52e-04 | gnorm 2.744 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.2202 | top1 48.34% | top3 68.74% | top5 76.84% | lr 4.52e-04 | grad_norm 2.744 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.6595 | top1 54.78% | top3 76.80% | top5 84.58%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.20 min\n",
            "\n",
            "=== Epoch 13/50 ===\n",
            "[train step 100/704] loss 2.1197 | top1 51.25% | top3 71.39% | top5 78.69% | 190.2 img/s | lr 4.51e-04 | gnorm 2.774 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.0787 | top1 51.16% | top3 71.53% | top5 79.05% | 191.1 img/s | lr 4.50e-04 | gnorm 2.781 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.0702 | top1 51.53% | top3 71.96% | top5 79.34% | 191.3 img/s | lr 4.48e-04 | gnorm 2.786 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.1217 | top1 50.63% | top3 70.92% | top5 78.43% | 191.3 img/s | lr 4.47e-04 | gnorm 2.738 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 500/704] loss 2.1462 | top1 50.29% | top3 70.53% | top5 78.21% | 191.4 img/s | lr 4.45e-04 | gnorm 2.719 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 2.1399 | top1 50.31% | top3 70.48% | top5 78.21% | 191.4 img/s | lr 4.44e-04 | gnorm 2.711 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 2.1396 | top1 50.27% | top3 70.58% | top5 78.26% | 191.4 img/s | lr 4.42e-04 | gnorm 2.708 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 2.1372 | top1 50.30% | top3 70.60% | top5 78.30% | 191.4 img/s | lr 4.42e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.1372 | top1 50.30% | top3 70.60% | top5 78.30% | lr 4.42e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.5931 | top1 57.10% | top3 77.66% | top5 84.62%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 57.10%)\n",
            "Epoch time: 4.35 min\n",
            "\n",
            "=== Epoch 14/50 ===\n",
            "[train step 100/704] loss 1.9766 | top1 53.19% | top3 73.33% | top5 80.81% | 188.0 img/s | lr 4.41e-04 | gnorm 2.721 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.0778 | top1 52.18% | top3 72.27% | top5 79.70% | 189.9 img/s | lr 4.39e-04 | gnorm 2.697 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.0801 | top1 52.20% | top3 72.49% | top5 79.93% | 190.6 img/s | lr 4.38e-04 | gnorm 2.690 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.0956 | top1 51.81% | top3 72.08% | top5 79.45% | 190.7 img/s | lr 4.36e-04 | gnorm 2.688 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.1167 | top1 51.52% | top3 71.76% | top5 79.25% | 190.8 img/s | lr 4.35e-04 | gnorm 2.674 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.1256 | top1 51.21% | top3 71.45% | top5 78.96% | 190.9 img/s | lr 4.33e-04 | gnorm 2.678 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.1169 | top1 51.53% | top3 71.78% | top5 79.26% | 191.0 img/s | lr 4.31e-04 | gnorm 2.687 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.1139 | top1 51.57% | top3 71.83% | top5 79.30% | 190.9 img/s | lr 4.31e-04 | gnorm 2.696 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.1139 | top1 51.57% | top3 71.83% | top5 79.30% | lr 4.31e-04 | grad_norm 2.696 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.5213 | top1 58.70% | top3 78.52% | top5 85.88%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 58.70%)\n",
            "Epoch time: 4.39 min\n",
            "\n",
            "=== Epoch 15/50 ===\n",
            "[train step 100/704] loss 2.0829 | top1 52.89% | top3 72.89% | top5 80.19% | 187.6 img/s | lr 4.30e-04 | gnorm 2.716 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.0139 | top1 53.81% | top3 73.31% | top5 80.36% | 189.8 img/s | lr 4.28e-04 | gnorm 2.769 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.0175 | top1 53.93% | top3 73.64% | top5 80.80% | 190.7 img/s | lr 4.26e-04 | gnorm 2.763 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 2.0087 | top1 53.99% | top3 73.72% | top5 80.85% | 190.7 img/s | lr 4.25e-04 | gnorm 2.779 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 2.0251 | top1 53.42% | top3 73.08% | top5 80.31% | 190.9 img/s | lr 4.23e-04 | gnorm 2.787 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.0348 | top1 53.43% | top3 73.15% | top5 80.52% | 191.0 img/s | lr 4.21e-04 | gnorm 2.779 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 2.0071 | top1 53.88% | top3 73.71% | top5 81.01% | 191.1 img/s | lr 4.20e-04 | gnorm 2.782 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 2.0039 | top1 53.91% | top3 73.77% | top5 81.06% | 191.0 img/s | lr 4.19e-04 | gnorm 2.787 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 2.0039 | top1 53.91% | top3 73.77% | top5 81.06% | lr 4.19e-04 | grad_norm 2.787 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.4448 | top1 60.44% | top3 80.42% | top5 87.02%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 60.44%)\n",
            "Epoch time: 4.31 min\n",
            "\n",
            "=== Epoch 16/50 ===\n",
            "[train step 100/704] loss 2.0282 | top1 54.64% | top3 73.91% | top5 80.92% | 188.8 img/s | lr 4.18e-04 | gnorm 2.694 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 2.0102 | top1 54.52% | top3 73.66% | top5 80.86% | 190.2 img/s | lr 4.16e-04 | gnorm 2.725 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 2.0541 | top1 53.90% | top3 73.52% | top5 80.70% | 190.7 img/s | lr 4.14e-04 | gnorm 2.698 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.9916 | top1 54.71% | top3 74.47% | top5 81.64% | 190.8 img/s | lr 4.12e-04 | gnorm 2.723 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.9905 | top1 54.83% | top3 74.52% | top5 81.62% | 190.9 img/s | lr 4.11e-04 | gnorm 2.729 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 2.0291 | top1 54.00% | top3 73.71% | top5 80.89% | 190.9 img/s | lr 4.09e-04 | gnorm 2.718 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 2.0099 | top1 54.40% | top3 74.02% | top5 81.13% | 190.9 img/s | lr 4.07e-04 | gnorm 2.726 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 2.0086 | top1 54.43% | top3 74.05% | top5 81.16% | 190.8 img/s | lr 4.07e-04 | gnorm 2.731 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 2.0086 | top1 54.43% | top3 74.05% | top5 81.16% | lr 4.07e-04 | grad_norm 2.731 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.4429 | top1 60.12% | top3 80.34% | top5 86.86%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.20 min\n",
            "\n",
            "=== Epoch 17/50 ===\n",
            "[train step 100/704] loss 1.8625 | top1 57.05% | top3 75.64% | top5 82.03% | 189.5 img/s | lr 4.05e-04 | gnorm 2.761 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 200/704] loss 1.8947 | top1 56.98% | top3 75.90% | top5 82.65% | 190.7 img/s | lr 4.03e-04 | gnorm 2.785 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 1.9415 | top1 56.15% | top3 75.46% | top5 82.16% | 190.9 img/s | lr 4.01e-04 | gnorm 2.771 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 1.9201 | top1 56.62% | top3 75.86% | top5 82.61% | 191.0 img/s | lr 4.00e-04 | gnorm 2.776 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 500/704] loss 1.9509 | top1 56.03% | top3 75.31% | top5 82.21% | 191.1 img/s | lr 3.98e-04 | gnorm 2.772 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 1.9578 | top1 55.95% | top3 75.14% | top5 82.02% | 191.1 img/s | lr 3.96e-04 | gnorm 2.755 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 1.9562 | top1 55.90% | top3 75.07% | top5 81.95% | 191.1 img/s | lr 3.94e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.9546 | top1 55.94% | top3 75.11% | top5 81.98% | 191.1 img/s | lr 3.94e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.9546 | top1 55.94% | top3 75.11% | top5 81.98% | lr 3.94e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.4256 | top1 60.74% | top3 80.72% | top5 87.28%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 60.74%)\n",
            "Epoch time: 4.22 min\n",
            "\n",
            "=== Epoch 18/50 ===\n",
            "[train step 100/704] loss 1.9069 | top1 57.73% | top3 77.84% | top5 84.05% | 189.6 img/s | lr 3.92e-04 | gnorm 2.711 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.8432 | top1 58.34% | top3 77.98% | top5 84.25% | 190.3 img/s | lr 3.90e-04 | gnorm 2.737 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.9109 | top1 57.19% | top3 76.49% | top5 82.98% | 190.6 img/s | lr 3.88e-04 | gnorm 2.717 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.8978 | top1 57.67% | top3 76.73% | top5 83.26% | 190.9 img/s | lr 3.86e-04 | gnorm 2.732 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.9181 | top1 56.95% | top3 76.09% | top5 82.68% | 190.9 img/s | lr 3.84e-04 | gnorm 2.717 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.8929 | top1 57.28% | top3 76.40% | top5 82.93% | 191.0 img/s | lr 3.82e-04 | gnorm 2.738 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.8910 | top1 57.11% | top3 76.24% | top5 82.79% | 191.1 img/s | lr 3.80e-04 | gnorm 2.746 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.8957 | top1 57.07% | top3 76.19% | top5 82.75% | 191.1 img/s | lr 3.80e-04 | gnorm 2.754 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.8957 | top1 57.07% | top3 76.19% | top5 82.75% | lr 3.80e-04 | grad_norm 2.754 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.4034 | top1 61.38% | top3 81.10% | top5 87.86%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 61.38%)\n",
            "Epoch time: 4.23 min\n",
            "\n",
            "=== Epoch 19/50 ===\n",
            "[train step 100/704] loss 1.7630 | top1 61.34% | top3 80.45% | top5 86.11% | 189.1 img/s | lr 3.78e-04 | gnorm 2.733 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.8244 | top1 60.34% | top3 79.20% | top5 85.51% | 190.5 img/s | lr 3.76e-04 | gnorm 2.755 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.8526 | top1 59.36% | top3 77.91% | top5 84.28% | 190.8 img/s | lr 3.74e-04 | gnorm 2.748 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.8840 | top1 58.42% | top3 77.23% | top5 83.66% | 190.8 img/s | lr 3.72e-04 | gnorm 2.731 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.8641 | top1 58.50% | top3 77.26% | top5 83.68% | 190.8 img/s | lr 3.70e-04 | gnorm 2.747 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.8928 | top1 58.07% | top3 76.79% | top5 83.19% | 191.0 img/s | lr 3.68e-04 | gnorm 2.740 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.8980 | top1 57.81% | top3 76.61% | top5 83.11% | 191.1 img/s | lr 3.66e-04 | gnorm 2.737 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.8995 | top1 57.79% | top3 76.59% | top5 83.09% | 191.0 img/s | lr 3.66e-04 | gnorm 2.746 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.8995 | top1 57.79% | top3 76.59% | top5 83.09% | lr 3.66e-04 | grad_norm 2.746 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.3798 | top1 62.50% | top3 82.44% | top5 88.72%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 62.50%)\n",
            "Epoch time: 4.26 min\n",
            "\n",
            "=== Epoch 20/50 ===\n",
            "[train step 100/704] loss 1.8723 | top1 59.34% | top3 77.86% | top5 83.97% | 189.3 img/s | lr 3.64e-04 | gnorm 2.719 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.8101 | top1 60.22% | top3 78.88% | top5 84.95% | 190.4 img/s | lr 3.61e-04 | gnorm 2.759 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.7766 | top1 60.55% | top3 78.78% | top5 84.90% | 190.7 img/s | lr 3.59e-04 | gnorm 2.780 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.7841 | top1 60.54% | top3 78.80% | top5 84.89% | 190.8 img/s | lr 3.57e-04 | gnorm 2.773 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.7945 | top1 60.22% | top3 78.47% | top5 84.62% | 191.0 img/s | lr 3.55e-04 | gnorm 2.762 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.7747 | top1 60.42% | top3 78.66% | top5 84.85% | 191.0 img/s | lr 3.53e-04 | gnorm 2.769 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 1.7880 | top1 60.19% | top3 78.48% | top5 84.70% | 191.0 img/s | lr 3.51e-04 | gnorm 2.777 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 1.7900 | top1 60.21% | top3 78.48% | top5 84.69% | 191.0 img/s | lr 3.51e-04 | gnorm 2.783 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 1.7900 | top1 60.21% | top3 78.48% | top5 84.69% | lr 3.51e-04 | grad_norm 2.783 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.3775 | top1 62.20% | top3 81.66% | top5 87.90%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.58 min\n",
            "\n",
            "=== Epoch 21/50 ===\n",
            "[train step 100/704] loss 1.8547 | top1 61.17% | top3 78.53% | top5 84.73% | 188.6 img/s | lr 3.49e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.7624 | top1 61.30% | top3 78.69% | top5 84.66% | 190.3 img/s | lr 3.46e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.7664 | top1 61.18% | top3 79.02% | top5 84.92% | 190.7 img/s | lr 3.44e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.7694 | top1 60.72% | top3 78.73% | top5 84.57% | 190.6 img/s | lr 3.42e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.7333 | top1 61.23% | top3 79.33% | top5 85.14% | 190.6 img/s | lr 3.40e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.7302 | top1 61.35% | top3 79.41% | top5 85.23% | 190.7 img/s | lr 3.38e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.7150 | top1 61.46% | top3 79.56% | top5 85.38% | 190.7 img/s | lr 3.35e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.7148 | top1 61.45% | top3 79.55% | top5 85.38% | 190.7 img/s | lr 3.35e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.7148 | top1 61.45% | top3 79.55% | top5 85.38% | lr 3.35e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.2822 | top1 64.62% | top3 83.60% | top5 89.44%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 64.62%)\n",
            "Epoch time: 4.30 min\n",
            "\n",
            "=== Epoch 22/50 ===\n",
            "[train step 100/704] loss 1.6567 | top1 63.47% | top3 80.97% | top5 86.55% | 188.5 img/s | lr 3.33e-04 | gnorm 2.751 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.6920 | top1 62.29% | top3 80.23% | top5 85.94% | 190.5 img/s | lr 3.31e-04 | gnorm 2.762 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.7124 | top1 61.96% | top3 79.90% | top5 85.55% | 190.8 img/s | lr 3.29e-04 | gnorm 2.774 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.7404 | top1 61.39% | top3 79.41% | top5 85.16% | 190.7 img/s | lr 3.27e-04 | gnorm 2.778 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.7355 | top1 61.55% | top3 79.57% | top5 85.39% | 190.9 img/s | lr 3.24e-04 | gnorm 2.782 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.7016 | top1 62.17% | top3 80.02% | top5 85.89% | 191.0 img/s | lr 3.22e-04 | gnorm 2.797 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.6852 | top1 62.40% | top3 80.15% | top5 85.95% | 191.1 img/s | lr 3.20e-04 | gnorm 2.811 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.6845 | top1 62.43% | top3 80.17% | top5 85.97% | 191.0 img/s | lr 3.20e-04 | gnorm 2.813 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.6845 | top1 62.43% | top3 80.17% | top5 85.97% | lr 3.20e-04 | grad_norm 2.813 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.2425 | top1 65.32% | top3 84.46% | top5 89.74%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 65.32%)\n",
            "Epoch time: 4.52 min\n",
            "\n",
            "=== Epoch 23/50 ===\n",
            "[train step 100/704] loss 1.6384 | top1 65.44% | top3 82.16% | top5 87.45% | 188.5 img/s | lr 3.17e-04 | gnorm 2.745 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.6077 | top1 64.89% | top3 82.16% | top5 87.38% | 190.0 img/s | lr 3.15e-04 | gnorm 2.811 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.5977 | top1 64.81% | top3 81.88% | top5 87.30% | 190.7 img/s | lr 3.13e-04 | gnorm 2.831 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.5948 | top1 64.53% | top3 81.61% | top5 87.11% | 190.8 img/s | lr 3.11e-04 | gnorm 2.841 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.6071 | top1 64.40% | top3 81.49% | top5 87.02% | 190.9 img/s | lr 3.08e-04 | gnorm 2.841 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.6089 | top1 64.44% | top3 81.56% | top5 87.05% | 191.1 img/s | lr 3.06e-04 | gnorm 2.835 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.6217 | top1 64.13% | top3 81.26% | top5 86.91% | 191.1 img/s | lr 3.04e-04 | gnorm 2.836 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 1.6218 | top1 64.14% | top3 81.26% | top5 86.91% | 191.0 img/s | lr 3.04e-04 | gnorm 2.839 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[Train] loss 1.6218 | top1 64.14% | top3 81.26% | top5 86.91% | lr 3.04e-04 | grad_norm 2.839 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 131072.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.2128 | top1 67.14% | top3 84.98% | top5 90.32%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 67.14%)\n",
            "Epoch time: 4.29 min\n",
            "\n",
            "=== Epoch 24/50 ===\n",
            "[train step 100/704] loss 1.6610 | top1 63.98% | top3 81.16% | top5 86.59% | 188.0 img/s | lr 3.01e-04 | gnorm 2.774 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 200/704] loss 1.5952 | top1 65.61% | top3 82.25% | top5 87.68% | 189.9 img/s | lr 2.99e-04 | gnorm 2.790 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 1.6176 | top1 64.57% | top3 81.55% | top5 86.99% | 190.4 img/s | lr 2.97e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.6017 | top1 64.63% | top3 81.79% | top5 87.22% | 190.7 img/s | lr 2.95e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.6222 | top1 64.29% | top3 81.47% | top5 86.90% | 190.9 img/s | lr 2.92e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.6255 | top1 64.53% | top3 81.79% | top5 87.18% | 191.0 img/s | lr 2.90e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.6347 | top1 64.41% | top3 81.71% | top5 87.06% | 191.0 img/s | lr 2.88e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.6370 | top1 64.37% | top3 81.68% | top5 87.04% | 191.0 img/s | lr 2.87e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.6370 | top1 64.37% | top3 81.68% | top5 87.04% | lr 2.87e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.2022 | top1 67.44% | top3 84.88% | top5 89.86%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 67.44%)\n",
            "Epoch time: 4.24 min\n",
            "\n",
            "=== Epoch 25/50 ===\n",
            "[train step 100/704] loss 1.6141 | top1 66.58% | top3 83.28% | top5 88.11% | 189.0 img/s | lr 2.85e-04 | gnorm 2.785 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.6081 | top1 65.73% | top3 82.54% | top5 87.60% | 190.6 img/s | lr 2.83e-04 | gnorm 2.822 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.6234 | top1 65.21% | top3 82.13% | top5 87.30% | 191.0 img/s | lr 2.81e-04 | gnorm 2.823 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.5927 | top1 65.67% | top3 82.38% | top5 87.47% | 191.0 img/s | lr 2.78e-04 | gnorm 2.827 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.6020 | top1 65.26% | top3 82.12% | top5 87.26% | 191.1 img/s | lr 2.76e-04 | gnorm 2.818 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.5693 | top1 65.76% | top3 82.48% | top5 87.58% | 191.1 img/s | lr 2.74e-04 | gnorm 2.837 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.5678 | top1 65.72% | top3 82.52% | top5 87.65% | 191.1 img/s | lr 2.71e-04 | gnorm 2.839 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.5672 | top1 65.71% | top3 82.53% | top5 87.65% | 191.1 img/s | lr 2.71e-04 | gnorm 2.847 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.5672 | top1 65.71% | top3 82.53% | top5 87.65% | lr 2.71e-04 | grad_norm 2.847 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.1770 | top1 67.94% | top3 85.78% | top5 91.16%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 67.94%)\n",
            "Epoch time: 4.35 min\n",
            "\n",
            "=== Epoch 26/50 ===\n",
            "[train step 100/704] loss 1.6181 | top1 65.36% | top3 82.16% | top5 87.27% | 187.7 img/s | lr 2.69e-04 | gnorm 2.849 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.5642 | top1 66.23% | top3 82.42% | top5 87.67% | 189.6 img/s | lr 2.66e-04 | gnorm 2.843 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.5584 | top1 65.80% | top3 82.09% | top5 87.27% | 190.3 img/s | lr 2.64e-04 | gnorm 2.838 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.5090 | top1 66.81% | top3 83.00% | top5 87.94% | 190.4 img/s | lr 2.62e-04 | gnorm 2.857 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.5224 | top1 66.72% | top3 83.01% | top5 87.93% | 190.6 img/s | lr 2.59e-04 | gnorm 2.858 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.5233 | top1 66.62% | top3 82.95% | top5 87.85% | 190.8 img/s | lr 2.57e-04 | gnorm 2.861 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.5312 | top1 66.52% | top3 82.89% | top5 87.79% | 190.8 img/s | lr 2.55e-04 | gnorm 2.876 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.5311 | top1 66.49% | top3 82.86% | top5 87.76% | 190.8 img/s | lr 2.55e-04 | gnorm 2.880 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.5311 | top1 66.49% | top3 82.86% | top5 87.76% | lr 2.55e-04 | grad_norm 2.880 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.1685 | top1 67.40% | top3 85.36% | top5 90.64%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.21 min\n",
            "\n",
            "=== Epoch 27/50 ===\n",
            "[train step 100/704] loss 1.4569 | top1 67.36% | top3 83.22% | top5 88.17% | 188.9 img/s | lr 2.52e-04 | gnorm 2.814 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.4028 | top1 69.19% | top3 84.80% | top5 89.47% | 190.3 img/s | lr 2.50e-04 | gnorm 2.847 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 300/704] loss 1.4445 | top1 68.82% | top3 84.47% | top5 89.34% | 190.6 img/s | lr 2.48e-04 | gnorm 2.873 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 1.4747 | top1 68.43% | top3 84.30% | top5 89.10% | 190.9 img/s | lr 2.45e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.4919 | top1 67.94% | top3 83.99% | top5 88.76% | 190.9 img/s | lr 2.43e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.4831 | top1 68.04% | top3 84.16% | top5 88.93% | 190.9 img/s | lr 2.41e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.4626 | top1 68.43% | top3 84.54% | top5 89.23% | 191.0 img/s | lr 2.38e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.4650 | top1 68.39% | top3 84.52% | top5 89.22% | 190.9 img/s | lr 2.38e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.4650 | top1 68.39% | top3 84.52% | top5 89.22% | lr 2.38e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.1145 | top1 69.16% | top3 86.38% | top5 91.60%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 69.16%)\n",
            "Epoch time: 4.43 min\n",
            "\n",
            "=== Epoch 28/50 ===\n",
            "[train step 100/704] loss 1.4043 | top1 70.59% | top3 86.19% | top5 90.50% | 188.0 img/s | lr 2.36e-04 | gnorm 2.852 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.4919 | top1 68.62% | top3 84.12% | top5 89.02% | 190.1 img/s | lr 2.33e-04 | gnorm 2.808 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.4521 | top1 69.83% | top3 85.10% | top5 89.65% | 190.8 img/s | lr 2.31e-04 | gnorm 2.826 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.4159 | top1 70.38% | top3 85.62% | top5 90.02% | 190.8 img/s | lr 2.29e-04 | gnorm 2.883 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.3924 | top1 70.83% | top3 85.84% | top5 90.17% | 191.1 img/s | lr 2.26e-04 | gnorm 2.903 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.4280 | top1 70.11% | top3 85.27% | top5 89.75% | 191.1 img/s | lr 2.24e-04 | gnorm 2.895 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.4070 | top1 70.49% | top3 85.63% | top5 90.00% | 191.1 img/s | lr 2.22e-04 | gnorm 2.899 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.4040 | top1 70.51% | top3 85.67% | top5 90.02% | 191.1 img/s | lr 2.22e-04 | gnorm 2.905 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.4040 | top1 70.51% | top3 85.67% | top5 90.02% | lr 2.22e-04 | grad_norm 2.905 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0919 | top1 69.34% | top3 86.56% | top5 91.36%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 69.34%)\n",
            "Epoch time: 4.30 min\n",
            "\n",
            "=== Epoch 29/50 ===\n",
            "[train step 100/704] loss 1.4185 | top1 70.62% | top3 85.66% | top5 89.80% | 189.5 img/s | lr 2.19e-04 | gnorm 2.792 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.5423 | top1 68.43% | top3 83.99% | top5 88.80% | 190.6 img/s | lr 2.17e-04 | gnorm 2.792 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.5285 | top1 68.64% | top3 83.89% | top5 88.48% | 191.0 img/s | lr 2.15e-04 | gnorm 2.786 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.5193 | top1 68.29% | top3 83.72% | top5 88.39% | 191.2 img/s | lr 2.12e-04 | gnorm 2.829 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.5287 | top1 68.30% | top3 83.76% | top5 88.42% | 191.4 img/s | lr 2.10e-04 | gnorm 2.831 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.5401 | top1 68.28% | top3 83.79% | top5 88.46% | 191.4 img/s | lr 2.08e-04 | gnorm 2.830 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.5228 | top1 68.62% | top3 84.08% | top5 88.69% | 191.3 img/s | lr 2.05e-04 | gnorm 2.844 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.5224 | top1 68.64% | top3 84.09% | top5 88.70% | 191.3 img/s | lr 2.05e-04 | gnorm 2.848 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.5224 | top1 68.64% | top3 84.09% | top5 88.70% | lr 2.05e-04 | grad_norm 2.848 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.1406 | top1 68.38% | top3 86.28% | top5 91.32%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.19 min\n",
            "\n",
            "=== Epoch 30/50 ===\n",
            "[train step 100/704] loss 1.4312 | top1 70.25% | top3 85.20% | top5 89.28% | 189.9 img/s | lr 2.03e-04 | gnorm 2.792 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.3230 | top1 72.04% | top3 86.32% | top5 90.32% | 191.0 img/s | lr 2.01e-04 | gnorm 2.880 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.3635 | top1 71.52% | top3 85.86% | top5 89.95% | 191.0 img/s | lr 1.98e-04 | gnorm 2.871 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 400/704] loss 1.3444 | top1 72.03% | top3 86.36% | top5 90.38% | 191.1 img/s | lr 1.96e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.3354 | top1 72.31% | top3 86.66% | top5 90.69% | 191.2 img/s | lr 1.94e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.3466 | top1 71.87% | top3 86.39% | top5 90.51% | 191.2 img/s | lr 1.92e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.3714 | top1 71.59% | top3 86.26% | top5 90.41% | 191.2 img/s | lr 1.89e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.3763 | top1 71.51% | top3 86.21% | top5 90.37% | 191.1 img/s | lr 1.89e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.3763 | top1 71.51% | top3 86.21% | top5 90.37% | lr 1.89e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.1196 | top1 70.10% | top3 86.92% | top5 91.48%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 70.10%)\n",
            "Epoch time: 4.31 min\n",
            "\n",
            "=== Epoch 31/50 ===\n",
            "[train step 100/704] loss 1.4358 | top1 71.52% | top3 85.28% | top5 89.59% | 188.7 img/s | lr 1.87e-04 | gnorm 2.813 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.3013 | top1 73.62% | top3 87.25% | top5 91.17% | 190.2 img/s | lr 1.85e-04 | gnorm 2.881 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.3615 | top1 72.48% | top3 86.50% | top5 90.65% | 190.4 img/s | lr 1.82e-04 | gnorm 2.880 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.3658 | top1 72.36% | top3 86.36% | top5 90.56% | 190.5 img/s | lr 1.80e-04 | gnorm 2.882 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.3316 | top1 72.72% | top3 86.66% | top5 90.72% | 190.7 img/s | lr 1.78e-04 | gnorm 2.882 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.3387 | top1 72.61% | top3 86.61% | top5 90.65% | 190.8 img/s | lr 1.76e-04 | gnorm 2.885 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.3317 | top1 72.77% | top3 86.84% | top5 90.81% | 190.9 img/s | lr 1.73e-04 | gnorm 2.904 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.3302 | top1 72.82% | top3 86.87% | top5 90.83% | 190.8 img/s | lr 1.73e-04 | gnorm 2.907 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.3302 | top1 72.82% | top3 86.87% | top5 90.83% | lr 1.73e-04 | grad_norm 2.907 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0356 | top1 70.38% | top3 88.34% | top5 92.78%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 70.38%)\n",
            "Epoch time: 4.32 min\n",
            "\n",
            "=== Epoch 32/50 ===\n",
            "[train step 100/704] loss 1.3564 | top1 72.56% | top3 86.53% | top5 90.38% | 188.5 img/s | lr 1.71e-04 | gnorm 2.821 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.2997 | top1 73.85% | top3 87.45% | top5 91.34% | 190.3 img/s | lr 1.69e-04 | gnorm 2.864 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.2584 | top1 74.64% | top3 88.10% | top5 91.74% | 190.8 img/s | lr 1.67e-04 | gnorm 2.895 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.2912 | top1 73.87% | top3 87.61% | top5 91.47% | 190.8 img/s | lr 1.65e-04 | gnorm 2.899 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.2869 | top1 73.96% | top3 87.80% | top5 91.58% | 190.9 img/s | lr 1.62e-04 | gnorm 2.907 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.3007 | top1 73.64% | top3 87.54% | top5 91.37% | 191.0 img/s | lr 1.60e-04 | gnorm 2.908 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.3067 | top1 73.55% | top3 87.48% | top5 91.33% | 191.0 img/s | lr 1.58e-04 | gnorm 2.913 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.3058 | top1 73.59% | top3 87.51% | top5 91.36% | 191.0 img/s | lr 1.58e-04 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 32768.0\n",
            "[Train] loss 1.3058 | top1 73.59% | top3 87.51% | top5 91.36% | lr 1.58e-04 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 32768.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0337 | top1 71.98% | top3 88.04% | top5 92.20%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 71.98%)\n",
            "Epoch time: 4.22 min\n",
            "\n",
            "=== Epoch 33/50 ===\n",
            "[train step 100/704] loss 1.2427 | top1 74.80% | top3 87.41% | top5 90.88% | 189.7 img/s | lr 1.56e-04 | gnorm 2.890 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 200/704] loss 1.2528 | top1 75.10% | top3 88.04% | top5 91.49% | 190.6 img/s | lr 1.54e-04 | gnorm 2.858 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 300/704] loss 1.2568 | top1 75.25% | top3 88.16% | top5 91.68% | 190.8 img/s | lr 1.51e-04 | gnorm 2.876 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 400/704] loss 1.2732 | top1 74.80% | top3 87.88% | top5 91.45% | 191.0 img/s | lr 1.49e-04 | gnorm 2.876 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 500/704] loss 1.2861 | top1 74.53% | top3 87.78% | top5 91.41% | 191.1 img/s | lr 1.47e-04 | gnorm 2.869 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 600/704] loss 1.2954 | top1 74.58% | top3 87.79% | top5 91.43% | 191.1 img/s | lr 1.45e-04 | gnorm 2.865 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 700/704] loss 1.2920 | top1 74.67% | top3 87.91% | top5 91.58% | 191.2 img/s | lr 1.43e-04 | gnorm 2.871 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 704/704] loss 1.2982 | top1 74.57% | top3 87.84% | top5 91.53% | 191.1 img/s | lr 1.43e-04 | gnorm 2.874 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[Train] loss 1.2982 | top1 74.57% | top3 87.84% | top5 91.53% | lr 1.43e-04 | grad_norm 2.874 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 32768.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0639 | top1 71.54% | top3 87.44% | top5 92.36%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.15 min\n",
            "\n",
            "=== Epoch 34/50 ===\n",
            "[train step 100/704] loss 1.4388 | top1 73.03% | top3 86.70% | top5 90.64% | 189.8 img/s | lr 1.41e-04 | gnorm 2.759 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 200/704] loss 1.3649 | top1 73.57% | top3 86.95% | top5 90.74% | 190.3 img/s | lr 1.39e-04 | gnorm 2.802 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 300/704] loss 1.3237 | top1 74.33% | top3 87.39% | top5 91.07% | 190.6 img/s | lr 1.36e-04 | gnorm 2.801 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 400/704] loss 1.2819 | top1 75.06% | top3 88.04% | top5 91.59% | 190.7 img/s | lr 1.34e-04 | gnorm 2.843 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 500/704] loss 1.2878 | top1 74.87% | top3 87.88% | top5 91.52% | 190.5 img/s | lr 1.32e-04 | gnorm 2.861 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 600/704] loss 1.2832 | top1 74.99% | top3 87.93% | top5 91.60% | 190.5 img/s | lr 1.30e-04 | gnorm 2.861 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 700/704] loss 1.2798 | top1 75.00% | top3 87.87% | top5 91.57% | 190.5 img/s | lr 1.28e-04 | gnorm 2.874 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 704/704] loss 1.2772 | top1 75.05% | top3 87.91% | top5 91.60% | 190.5 img/s | lr 1.28e-04 | gnorm 2.883 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[Train] loss 1.2772 | top1 75.05% | top3 87.91% | top5 91.60% | lr 1.28e-04 | grad_norm 2.883 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 32768.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0159 | top1 72.10% | top3 87.44% | top5 92.28%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 72.10%)\n",
            "Epoch time: 4.27 min\n",
            "\n",
            "=== Epoch 35/50 ===\n",
            "[train step 100/704] loss 1.1344 | top1 79.19% | top3 90.20% | top5 93.52% | 184.4 img/s | lr 1.26e-04 | gnorm 2.787 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 200/704] loss 1.2730 | top1 75.92% | top3 87.84% | top5 91.72% | 187.9 img/s | lr 1.24e-04 | gnorm 2.790 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 300/704] loss 1.2481 | top1 75.73% | top3 87.80% | top5 91.65% | 188.9 img/s | lr 1.22e-04 | gnorm 2.813 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 400/704] loss 1.2745 | top1 75.50% | top3 87.81% | top5 91.59% | 189.4 img/s | lr 1.20e-04 | gnorm 2.827 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 500/704] loss 1.3033 | top1 75.17% | top3 87.57% | top5 91.46% | 189.7 img/s | lr 1.18e-04 | gnorm 2.819 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 600/704] loss 1.2922 | top1 75.38% | top3 87.76% | top5 91.60% | 189.9 img/s | lr 1.16e-04 | gnorm 2.829 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.2835 | top1 75.30% | top3 87.72% | top5 91.52% | 190.0 img/s | lr 1.14e-04 | gnorm 2.847 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.2794 | top1 75.37% | top3 87.76% | top5 91.55% | 190.0 img/s | lr 1.14e-04 | gnorm 2.856 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.2794 | top1 75.37% | top3 87.76% | top5 91.55% | lr 1.14e-04 | grad_norm 2.856 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0152 | top1 71.68% | top3 88.30% | top5 92.64%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.24 min\n",
            "\n",
            "=== Epoch 36/50 ===\n",
            "[train step 100/704] loss 1.3269 | top1 75.00% | top3 87.84% | top5 91.39% | 188.5 img/s | lr 1.12e-04 | gnorm 2.794 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.2596 | top1 75.89% | top3 88.33% | top5 91.92% | 189.7 img/s | lr 1.10e-04 | gnorm 2.794 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.2013 | top1 76.32% | top3 88.44% | top5 91.93% | 190.2 img/s | lr 1.08e-04 | gnorm 2.838 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1865 | top1 76.51% | top3 88.52% | top5 91.95% | 190.5 img/s | lr 1.06e-04 | gnorm 2.828 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.1931 | top1 76.76% | top3 88.83% | top5 92.24% | 190.6 img/s | lr 1.04e-04 | gnorm 2.836 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.2163 | top1 76.19% | top3 88.53% | top5 91.99% | 190.8 img/s | lr 1.02e-04 | gnorm 2.863 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.2072 | top1 76.17% | top3 88.51% | top5 92.01% | 190.9 img/s | lr 1.01e-04 | gnorm 2.878 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.2056 | top1 76.21% | top3 88.54% | top5 92.03% | 190.8 img/s | lr 1.01e-04 | gnorm 2.886 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.2056 | top1 76.21% | top3 88.54% | top5 92.03% | lr 1.01e-04 | grad_norm 2.886 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 1.0016 | top1 72.60% | top3 88.24% | top5 92.52%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 72.60%)\n",
            "Epoch time: 4.37 min\n",
            "\n",
            "=== Epoch 37/50 ===\n",
            "[train step 100/704] loss 1.1982 | top1 78.06% | top3 88.92% | top5 92.52% | 188.0 img/s | lr 9.87e-05 | gnorm 2.799 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.2385 | top1 76.95% | top3 88.50% | top5 92.07% | 189.7 img/s | lr 9.68e-05 | gnorm 2.799 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.2438 | top1 76.61% | top3 88.47% | top5 92.09% | 190.3 img/s | lr 9.50e-05 | gnorm 2.800 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.2410 | top1 76.50% | top3 88.47% | top5 92.10% | 190.2 img/s | lr 9.31e-05 | gnorm 2.815 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.2316 | top1 76.57% | top3 88.44% | top5 92.04% | 190.3 img/s | lr 9.13e-05 | gnorm 2.829 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.2261 | top1 76.80% | top3 88.63% | top5 92.18% | 190.5 img/s | lr 8.95e-05 | gnorm 2.824 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.2088 | top1 77.13% | top3 88.92% | top5 92.33% | 190.6 img/s | lr 8.78e-05 | gnorm 2.838 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.2098 | top1 77.13% | top3 88.92% | top5 92.33% | 190.5 img/s | lr 8.77e-05 | gnorm 2.846 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.2098 | top1 77.13% | top3 88.92% | top5 92.33% | lr 8.77e-05 | grad_norm 2.846 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9829 | top1 72.66% | top3 88.66% | top5 92.84%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 72.66%)\n",
            "Epoch time: 4.23 min\n",
            "\n",
            "=== Epoch 38/50 ===\n",
            "[train step 100/704] loss 1.1584 | top1 79.16% | top3 89.47% | top5 92.98% | 189.7 img/s | lr 8.59e-05 | gnorm 2.798 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.1276 | top1 79.84% | top3 89.90% | top5 93.02% | 190.2 img/s | lr 8.42e-05 | gnorm 2.814 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1938 | top1 79.17% | top3 89.89% | top5 93.11% | 190.6 img/s | lr 8.24e-05 | gnorm 2.818 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1825 | top1 78.92% | top3 89.71% | top5 92.96% | 190.7 img/s | lr 8.07e-05 | gnorm 2.824 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.1547 | top1 78.82% | top3 89.75% | top5 92.96% | 190.8 img/s | lr 7.90e-05 | gnorm 2.840 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 1.1729 | top1 78.37% | top3 89.45% | top5 92.73% | 190.9 img/s | lr 7.73e-05 | gnorm 2.843 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 1.1754 | top1 78.21% | top3 89.29% | top5 92.63% | 191.0 img/s | lr 7.56e-05 | gnorm 2.842 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 704/704] loss 1.1777 | top1 78.18% | top3 89.28% | top5 92.62% | 191.0 img/s | lr 7.55e-05 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.1777 | top1 78.18% | top3 89.28% | top5 92.62% | lr 7.55e-05 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9622 | top1 73.00% | top3 88.94% | top5 92.76%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 73.00%)\n",
            "Epoch time: 4.41 min\n",
            "\n",
            "=== Epoch 39/50 ===\n",
            "[train step 100/704] loss 1.1005 | top1 79.47% | top3 90.05% | top5 93.38% | 188.9 img/s | lr 7.39e-05 | gnorm 2.767 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.0956 | top1 79.73% | top3 90.24% | top5 93.28% | 190.5 img/s | lr 7.22e-05 | gnorm 2.755 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1373 | top1 78.57% | top3 89.69% | top5 92.79% | 190.8 img/s | lr 7.06e-05 | gnorm 2.784 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1444 | top1 78.68% | top3 89.71% | top5 92.76% | 190.9 img/s | lr 6.90e-05 | gnorm 2.791 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.1544 | top1 78.49% | top3 89.63% | top5 92.71% | 191.0 img/s | lr 6.74e-05 | gnorm 2.794 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.1862 | top1 77.99% | top3 89.29% | top5 92.42% | 191.1 img/s | lr 6.58e-05 | gnorm 2.801 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.1786 | top1 78.04% | top3 89.27% | top5 92.42% | 191.0 img/s | lr 6.42e-05 | gnorm 2.808 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.1811 | top1 78.00% | top3 89.24% | top5 92.42% | 191.0 img/s | lr 6.42e-05 | gnorm 2.817 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.1811 | top1 78.00% | top3 89.24% | top5 92.42% | lr 6.42e-05 | grad_norm 2.817 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9901 | top1 72.82% | top3 88.88% | top5 93.06%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.21 min\n",
            "\n",
            "=== Epoch 40/50 ===\n",
            "[train step 100/704] loss 1.1411 | top1 80.41% | top3 90.41% | top5 93.00% | 189.2 img/s | lr 6.26e-05 | gnorm 2.728 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.0833 | top1 81.71% | top3 91.53% | top5 94.05% | 190.5 img/s | lr 6.11e-05 | gnorm 2.703 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1356 | top1 79.89% | top3 90.49% | top5 93.41% | 190.7 img/s | lr 5.96e-05 | gnorm 2.714 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1864 | top1 78.31% | top3 89.35% | top5 92.48% | 191.0 img/s | lr 5.81e-05 | gnorm 2.753 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.2246 | top1 77.49% | top3 88.92% | top5 92.17% | 191.1 img/s | lr 5.66e-05 | gnorm 2.778 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.2133 | top1 77.24% | top3 88.70% | top5 91.98% | 191.2 img/s | lr 5.51e-05 | gnorm 2.790 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.1765 | top1 77.84% | top3 89.13% | top5 92.26% | 191.3 img/s | lr 5.37e-05 | gnorm 2.798 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.1748 | top1 77.88% | top3 89.16% | top5 92.28% | 191.2 img/s | lr 5.36e-05 | gnorm inf | clip 100.0% | oflow 1 | nonfinite 0 | scale 32768.0\n",
            "[Train] loss 1.1748 | top1 77.88% | top3 89.16% | top5 92.28% | lr 5.36e-05 | grad_norm inf | clip 100.0% | amp_overflows 1 | nonfinite_loss 0 | scale 32768.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9620 | top1 73.38% | top3 89.24% | top5 93.20%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 73.38%)\n",
            "Epoch time: 4.30 min\n",
            "\n",
            "=== Epoch 41/50 ===\n",
            "[train step 100/704] loss 1.1560 | top1 77.94% | top3 89.70% | top5 92.91% | 189.2 img/s | lr 5.22e-05 | gnorm 2.809 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 200/704] loss 1.1174 | top1 79.28% | top3 90.34% | top5 93.54% | 190.5 img/s | lr 5.08e-05 | gnorm 2.796 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 300/704] loss 1.1323 | top1 79.40% | top3 90.45% | top5 93.61% | 190.7 img/s | lr 4.94e-05 | gnorm 2.788 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 400/704] loss 1.1327 | top1 79.15% | top3 90.18% | top5 93.28% | 190.8 img/s | lr 4.80e-05 | gnorm 2.805 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 500/704] loss 1.1122 | top1 79.28% | top3 90.17% | top5 93.14% | 190.9 img/s | lr 4.66e-05 | gnorm 2.802 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 600/704] loss 1.1235 | top1 79.35% | top3 90.20% | top5 93.14% | 191.0 img/s | lr 4.53e-05 | gnorm 2.804 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 700/704] loss 1.1148 | top1 79.66% | top3 90.45% | top5 93.33% | 191.0 img/s | lr 4.40e-05 | gnorm 2.806 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 704/704] loss 1.1147 | top1 79.64% | top3 90.44% | top5 93.33% | 191.0 img/s | lr 4.39e-05 | gnorm 2.816 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[Train] loss 1.1147 | top1 79.64% | top3 90.44% | top5 93.33% | lr 4.39e-05 | grad_norm 2.816 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 32768.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9465 | top1 73.64% | top3 88.98% | top5 93.14%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 73.64%)\n",
            "Epoch time: 4.28 min\n",
            "\n",
            "=== Epoch 42/50 ===\n",
            "[train step 100/704] loss 0.9449 | top1 82.25% | top3 91.59% | top5 93.77% | 189.5 img/s | lr 4.26e-05 | gnorm 2.694 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 200/704] loss 1.0099 | top1 81.35% | top3 91.05% | top5 93.66% | 190.9 img/s | lr 4.13e-05 | gnorm 2.735 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 300/704] loss 1.0812 | top1 80.51% | top3 90.70% | top5 93.56% | 191.1 img/s | lr 4.01e-05 | gnorm 2.747 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 400/704] loss 1.0999 | top1 80.20% | top3 90.50% | top5 93.38% | 191.1 img/s | lr 3.88e-05 | gnorm 2.743 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 500/704] loss 1.1162 | top1 80.10% | top3 90.51% | top5 93.41% | 191.2 img/s | lr 3.76e-05 | gnorm 2.760 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 600/704] loss 1.1045 | top1 80.28% | top3 90.48% | top5 93.35% | 191.3 img/s | lr 3.64e-05 | gnorm 2.751 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 700/704] loss 1.1223 | top1 79.58% | top3 89.96% | top5 92.93% | 191.2 img/s | lr 3.52e-05 | gnorm 2.749 | clip 99.9% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 704/704] loss 1.1223 | top1 79.61% | top3 89.99% | top5 92.95% | 191.1 img/s | lr 3.51e-05 | gnorm 2.760 | clip 99.9% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[Train] loss 1.1223 | top1 79.61% | top3 89.99% | top5 92.95% | lr 3.51e-05 | grad_norm 2.760 | clip 99.9% | amp_overflows 0 | nonfinite_loss 0 | scale 32768.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9555 | top1 73.40% | top3 89.16% | top5 93.26%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.22 min\n",
            "\n",
            "=== Epoch 43/50 ===\n",
            "[train step 100/704] loss 1.1616 | top1 79.22% | top3 89.19% | top5 92.45% | 189.5 img/s | lr 3.39e-05 | gnorm 2.748 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 200/704] loss 1.2349 | top1 77.48% | top3 88.23% | top5 91.62% | 190.6 img/s | lr 3.28e-05 | gnorm 2.775 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 300/704] loss 1.1935 | top1 78.40% | top3 88.97% | top5 92.28% | 191.0 img/s | lr 3.17e-05 | gnorm 2.754 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 400/704] loss 1.1327 | top1 79.64% | top3 89.80% | top5 92.84% | 191.0 img/s | lr 3.05e-05 | gnorm 2.744 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 500/704] loss 1.1185 | top1 79.87% | top3 89.97% | top5 92.95% | 191.1 img/s | lr 2.94e-05 | gnorm 2.756 | clip 100.0% | oflow 0 | nonfinite 0 | scale 32768.0\n",
            "[train step 600/704] loss 1.1302 | top1 79.62% | top3 89.96% | top5 92.97% | 191.2 img/s | lr 2.84e-05 | gnorm 2.759 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.0924 | top1 80.24% | top3 90.31% | top5 93.24% | 191.2 img/s | lr 2.73e-05 | gnorm 2.751 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.0961 | top1 80.18% | top3 90.29% | top5 93.22% | 191.1 img/s | lr 2.73e-05 | gnorm 2.757 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.0961 | top1 80.18% | top3 90.29% | top5 93.22% | lr 2.73e-05 | grad_norm 2.757 | clip 100.0% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9423 | top1 73.78% | top3 89.26% | top5 93.48%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 73.78%)\n",
            "Epoch time: 4.32 min\n",
            "\n",
            "=== Epoch 44/50 ===\n",
            "[train step 100/704] loss 1.0139 | top1 83.33% | top3 92.25% | top5 94.41% | 188.8 img/s | lr 2.62e-05 | gnorm 2.677 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.1519 | top1 80.48% | top3 90.26% | top5 93.15% | 190.0 img/s | lr 2.52e-05 | gnorm 2.702 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1043 | top1 80.64% | top3 90.51% | top5 93.28% | 190.6 img/s | lr 2.42e-05 | gnorm 2.709 | clip 99.3% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1050 | top1 80.59% | top3 90.49% | top5 93.20% | 190.6 img/s | lr 2.32e-05 | gnorm 2.721 | clip 99.5% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.0864 | top1 80.61% | top3 90.44% | top5 93.14% | 190.8 img/s | lr 2.23e-05 | gnorm 2.735 | clip 99.6% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.0653 | top1 81.00% | top3 90.78% | top5 93.41% | 190.9 img/s | lr 2.13e-05 | gnorm 2.742 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.0660 | top1 80.97% | top3 90.69% | top5 93.41% | 190.9 img/s | lr 2.04e-05 | gnorm 2.737 | clip 99.6% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.0713 | top1 80.90% | top3 90.66% | top5 93.40% | 190.9 img/s | lr 2.04e-05 | gnorm 2.744 | clip 99.6% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.0713 | top1 80.90% | top3 90.66% | top5 93.40% | lr 2.04e-05 | grad_norm 2.744 | clip 99.6% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9687 | top1 74.78% | top3 89.12% | top5 93.10%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Best saved to best_maxout_medium.pt (val top1 74.78%)\n",
            "Epoch time: 5.16 min\n",
            "\n",
            "=== Epoch 45/50 ===\n",
            "[train step 100/704] loss 1.1645 | top1 77.17% | top3 88.30% | top5 91.78% | 188.9 img/s | lr 1.95e-05 | gnorm 2.724 | clip 99.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.0897 | top1 79.49% | top3 89.78% | top5 92.90% | 190.1 img/s | lr 1.86e-05 | gnorm 2.713 | clip 99.5% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.0634 | top1 80.38% | top3 90.33% | top5 93.18% | 190.6 img/s | lr 1.78e-05 | gnorm 2.720 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.0380 | top1 81.07% | top3 90.72% | top5 93.49% | 190.6 img/s | lr 1.69e-05 | gnorm 2.712 | clip 99.8% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.0487 | top1 80.87% | top3 90.62% | top5 93.40% | 190.8 img/s | lr 1.61e-05 | gnorm 2.688 | clip 99.6% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.0961 | top1 79.98% | top3 90.12% | top5 93.09% | 190.9 img/s | lr 1.53e-05 | gnorm 2.702 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.1030 | top1 79.93% | top3 90.09% | top5 93.02% | 190.9 img/s | lr 1.45e-05 | gnorm 2.697 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.1010 | top1 79.98% | top3 90.12% | top5 93.03% | 190.9 img/s | lr 1.45e-05 | gnorm 2.702 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.1010 | top1 79.98% | top3 90.12% | top5 93.03% | lr 1.45e-05 | grad_norm 2.702 | clip 99.7% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9200 | top1 74.76% | top3 89.92% | top5 93.86%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.21 min\n",
            "\n",
            "=== Epoch 46/50 ===\n",
            "[train step 100/704] loss 1.0915 | top1 79.56% | top3 89.86% | top5 92.72% | 189.4 img/s | lr 1.38e-05 | gnorm 2.674 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.1132 | top1 80.12% | top3 90.20% | top5 93.11% | 190.7 img/s | lr 1.30e-05 | gnorm 2.643 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1352 | top1 79.97% | top3 90.24% | top5 93.18% | 191.1 img/s | lr 1.23e-05 | gnorm 2.643 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1207 | top1 80.44% | top3 90.57% | top5 93.43% | 191.2 img/s | lr 1.16e-05 | gnorm 2.653 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.1148 | top1 80.62% | top3 90.60% | top5 93.45% | 191.1 img/s | lr 1.10e-05 | gnorm 2.660 | clip 100.0% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 1.1091 | top1 80.57% | top3 90.51% | top5 93.44% | 191.2 img/s | lr 1.03e-05 | gnorm 2.644 | clip 99.8% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 700/704] loss 1.1140 | top1 80.35% | top3 90.33% | top5 93.31% | 191.3 img/s | lr 9.70e-06 | gnorm inf | clip 99.7% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.1132 | top1 80.37% | top3 90.34% | top5 93.32% | 191.2 img/s | lr 9.68e-06 | gnorm inf | clip 99.7% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.1132 | top1 80.37% | top3 90.34% | top5 93.32% | lr 9.68e-06 | grad_norm inf | clip 99.7% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9175 | top1 74.28% | top3 89.80% | top5 93.58%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.22 min\n",
            "\n",
            "=== Epoch 47/50 ===\n",
            "[train step 100/704] loss 1.2448 | top1 79.03% | top3 89.89% | top5 93.11% | 189.5 img/s | lr 9.08e-06 | gnorm 2.621 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.1788 | top1 80.09% | top3 90.19% | top5 93.16% | 190.6 img/s | lr 8.50e-06 | gnorm 2.602 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1230 | top1 80.74% | top3 90.69% | top5 93.39% | 191.0 img/s | lr 7.94e-06 | gnorm 2.654 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.0623 | top1 82.02% | top3 91.55% | top5 94.09% | 191.1 img/s | lr 7.40e-06 | gnorm 2.665 | clip 99.8% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.0579 | top1 81.97% | top3 91.45% | top5 94.05% | 191.2 img/s | lr 6.88e-06 | gnorm 2.668 | clip 99.8% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.0876 | top1 81.51% | top3 91.24% | top5 93.89% | 191.2 img/s | lr 6.39e-06 | gnorm 2.671 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.1025 | top1 81.16% | top3 91.05% | top5 93.74% | 191.2 img/s | lr 5.91e-06 | gnorm 2.686 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.1016 | top1 81.17% | top3 91.05% | top5 93.74% | 191.2 img/s | lr 5.90e-06 | gnorm 2.689 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.1016 | top1 81.17% | top3 91.05% | top5 93.74% | lr 5.90e-06 | grad_norm 2.689 | clip 99.7% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9309 | top1 74.12% | top3 89.30% | top5 93.08%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.29 min\n",
            "\n",
            "=== Epoch 48/50 ===\n",
            "[train step 100/704] loss 0.8875 | top1 85.23% | top3 93.16% | top5 95.11% | 188.3 img/s | lr 5.44e-06 | gnorm 2.565 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 0.9405 | top1 84.16% | top3 92.77% | top5 94.93% | 189.9 img/s | lr 5.01e-06 | gnorm 2.578 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 0.9796 | top1 83.41% | top3 92.27% | top5 94.63% | 190.5 img/s | lr 4.61e-06 | gnorm 2.649 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 0.9972 | top1 83.29% | top3 92.13% | top5 94.54% | 190.7 img/s | lr 4.22e-06 | gnorm 2.624 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.0061 | top1 83.07% | top3 92.02% | top5 94.52% | 190.9 img/s | lr 3.86e-06 | gnorm 2.639 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.0315 | top1 82.12% | top3 91.40% | top5 94.07% | 190.9 img/s | lr 3.51e-06 | gnorm 2.649 | clip 99.8% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.0388 | top1 82.07% | top3 91.37% | top5 94.05% | 191.0 img/s | lr 3.19e-06 | gnorm 2.652 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.0422 | top1 82.03% | top3 91.34% | top5 94.03% | 190.9 img/s | lr 3.18e-06 | gnorm 2.660 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.0422 | top1 82.03% | top3 91.34% | top5 94.03% | lr 3.18e-06 | grad_norm 2.660 | clip 99.7% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9776 | top1 74.00% | top3 89.14% | top5 92.90%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.28 min\n",
            "\n",
            "=== Epoch 49/50 ===\n",
            "[train step 100/704] loss 1.1245 | top1 80.83% | top3 90.61% | top5 93.47% | 188.6 img/s | lr 2.88e-06 | gnorm 2.652 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 1.1020 | top1 81.58% | top3 91.12% | top5 93.90% | 190.2 img/s | lr 2.61e-06 | gnorm 2.634 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.1602 | top1 80.42% | top3 90.38% | top5 93.29% | 190.7 img/s | lr 2.35e-06 | gnorm 2.628 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.1779 | top1 79.87% | top3 89.99% | top5 92.97% | 190.9 img/s | lr 2.12e-06 | gnorm 2.620 | clip 99.8% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.1303 | top1 80.43% | top3 90.34% | top5 93.18% | 191.0 img/s | lr 1.91e-06 | gnorm 2.609 | clip 99.8% | oflow 0 | nonfinite 0 | scale 131072.0\n",
            "[train step 600/704] loss 1.1019 | top1 80.73% | top3 90.55% | top5 93.33% | 191.0 img/s | lr 1.72e-06 | gnorm inf | clip 99.8% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.0765 | top1 81.24% | top3 90.81% | top5 93.55% | 191.1 img/s | lr 1.55e-06 | gnorm inf | clip 99.9% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.0798 | top1 81.20% | top3 90.78% | top5 93.53% | 191.0 img/s | lr 1.55e-06 | gnorm inf | clip 99.9% | oflow 1 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.0798 | top1 81.20% | top3 90.78% | top5 93.53% | lr 1.55e-06 | grad_norm inf | clip 99.9% | amp_overflows 1 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9170 | top1 74.70% | top3 89.66% | top5 93.50%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.21 min\n",
            "\n",
            "=== Epoch 50/50 ===\n",
            "[train step 100/704] loss 1.1099 | top1 79.89% | top3 90.23% | top5 92.89% | 190.2 img/s | lr 1.40e-06 | gnorm 2.686 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 200/704] loss 0.9957 | top1 82.54% | top3 91.81% | top5 94.26% | 190.9 img/s | lr 1.28e-06 | gnorm 2.657 | clip 100.0% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 300/704] loss 1.0411 | top1 81.66% | top3 91.44% | top5 94.08% | 191.2 img/s | lr 1.18e-06 | gnorm 2.628 | clip 99.3% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 400/704] loss 1.0619 | top1 81.17% | top3 91.05% | top5 93.82% | 191.1 img/s | lr 1.10e-06 | gnorm 2.642 | clip 99.5% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 500/704] loss 1.0891 | top1 80.44% | top3 90.49% | top5 93.46% | 191.3 img/s | lr 1.05e-06 | gnorm 2.656 | clip 99.6% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 600/704] loss 1.0741 | top1 80.83% | top3 90.78% | top5 93.65% | 191.3 img/s | lr 1.01e-06 | gnorm 2.644 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 700/704] loss 1.0531 | top1 81.31% | top3 91.06% | top5 93.84% | 191.4 img/s | lr 1.00e-06 | gnorm 2.639 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[train step 704/704] loss 1.0548 | top1 81.26% | top3 91.01% | top5 93.80% | 191.3 img/s | lr 1.00e-06 | gnorm 2.645 | clip 99.7% | oflow 0 | nonfinite 0 | scale 65536.0\n",
            "[Train] loss 1.0548 | top1 81.26% | top3 91.01% | top5 93.80% | lr 1.00e-06 | grad_norm 2.645 | clip 99.7% | amp_overflows 0 | nonfinite_loss 0 | scale 65536.0\n",
            "[Train] mem_peak alloc 3.87 GiB | reserved 4.44 GiB\n",
            "[Val]   loss 0.9368 | top1 74.30% | top3 89.60% | top5 93.48%\n",
            "[Val]   mem_peak alloc 0.86 GiB | reserved 4.44 GiB\n",
            "Epoch time: 4.28 min\n"
          ]
        }
      ],
      "source": [
        "import random, numpy as np\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "random.seed(seed);\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "history, model = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    epochs=50,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "\n",
        "    lr=5e-4,\n",
        "    weight_decay=0.05,\n",
        "\n",
        "    autocast_dtype=\"fp16\" if device == \"cuda\" else \"fp32\",\n",
        "    use_amp=(device == \"cuda\"),\n",
        "    grad_clip_norm=1.0,\n",
        "\n",
        "    warmup_ratio=0.05,\n",
        "    min_lr=1e-6,\n",
        "\n",
        "    label_smoothing=0.0,\n",
        "\n",
        "    print_every=100,\n",
        "    save_path=\"best_maxout_medium.pt\",\n",
        "    last_path=\"last_maxout_medium.pt\",\n",
        "    resume_path=None,\n",
        "\n",
        "    # Augmentations\n",
        "    mix_prob=0.5,\n",
        "    mixup_alpha=0.0,\n",
        "    cutmix_alpha=1.0,\n",
        "\n",
        "    num_classes=100,\n",
        "    channels_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vGzF0rIwmqP",
        "outputId": "af1d6dc4-2bf3-4503-f02f-45d7b87a8f92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7801645481109619, {'top1': 78.42, 'top3': 92.07, 'top5': 95.22})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_one_epoch(model=model,dataloader=test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
